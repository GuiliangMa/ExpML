# æœºå™¨å­¦ä¹ åŸºç¡€ å®éªŒäºŒ å®éªŒæŠ¥å‘Š

2021çº§è½¯ä»¶5ç­ é©¬è´µäº® 202122202214

## å®éªŒç›®çš„

äº†è§£çº¿æ€§åˆ¤åˆ«å‡½æ•°ä¸å‚æ•°ã€éå‚æ•°ä¼°è®¡çš„ç›¸å…³çŸ¥è¯†ï¼Œå®Œæˆå¦‚ä¸‹å››ä¸ªä»»åŠ¡ã€‚

## å®éªŒå†…å®¹

å°†åœ¨åç»­å®éªŒè¿‡ç¨‹ä¸­ä¾æ¬¡é˜è¿°å¯¹åº”çš„å†…å®¹ã€‚

## å®éªŒè¿‡ç¨‹

### 1 çº¿æ€§åˆ¤åˆ«å‡½æ•°

#### 1.1å®éªŒå†…å®¹

é‡‡ç”¨`exp2_1.mat`ä¸­çš„æ•°æ®ï¼Œå®ç°çº¿æ€§åˆ¤åˆ«å‡½æ•°åˆ†ç±»ç®—æ³•ï¼Œå…¶`x1ã€x2`ä¸ºäºŒç»´è‡ªå˜é‡ï¼Œ`y`ä¸ºæ ·æœ¬ç±»åˆ«ã€‚ç¼–ç¨‹å®ç°çº¿æ€§åˆ¤åˆ«å‡½æ•°åˆ†ç±»ï¼Œå¹¶åšå‡ºåˆ†ç±»ç»“æœå¯è§†åŒ–ã€‚

#### 1.2 å®éªŒè¿‡ç¨‹

é¦–å…ˆæˆ‘éœ€è¦å¯¹æ•°æ®è¿›è¡Œè¯»å–ï¼Œç”±äºåŸå§‹æ•°æ®å­˜åœ¨åœ¨ä¸€ä¸ª`.mat` æ–‡ä»¶ä¸­ï¼Œå› æ­¤éœ€è¦å€ŸåŠ© `scipy` åº“å¯¹`.mat` æ–‡ä»¶è¿›è¡Œè¯»å…¥ï¼Œè¯»å…¥åä¸ºäº†åç»­å¤„ç†æ–¹ä¾¿ï¼Œæˆ‘åœ¨è¯¥ä»£ç ä¸­å°†å…¶è½¬æ¢ä¸º `dataFrame` æ¥ä¾¿äºæˆ‘åç»­çš„æ•°æ®å¤„ç†ã€‚

æœ‰å…³ä»£ç å¦‚ä¸‹ï¼š

```python
import scipy
import pandas as pd
data = scipy.io.loadmat('../../data/e2data1.mat')
X = pd.DataFrame(data['X'], columns=['x1', 'x2'])
y = pd.DataFrame(data['y'], columns=['y'])
```

æˆ‘ä»¬æ‰§è¡Œå®Œè¿™æ®µä»£ç åæˆ‘ä»¬ä¾¿æœ‰äº†ä¸€ç»„æ•°æ®ï¼Œé¦–å…ˆæ ¹æ® `y` å¯¹åº”çš„å€¼å°†æ‰€æœ‰çš„ `X` åˆ†æˆä¸¤ç±»ï¼Œç”±äºæ‰€æœ‰ `X` å‡ä¸ºäºŒç»´æ•£ç‚¹ï¼Œå› æ­¤å¯ä»¥æ ¹æ®ä¸åŒçš„é¢œè‰²ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œæ¥å®ç°å¯¹æ•°æ®çš„åŸºæœ¬å¯è§†åŒ–ã€‚

![](G:\ExpMachineLearn\ExpML\Exp2\images\part1æ•°æ®åˆ†å¸ƒ.png)

é€šè¿‡å¯¹æ•£ç‚¹åˆ†å¸ƒçš„å¤§è‡´è§‚å¯Ÿï¼Œè¯¥ç»„æ•°æ®å¤§è‡´çº¿æ€§å¯åˆ†ï¼Œå› æ­¤é‡‡ç”¨ç®€å•çš„çº¿æ€§åˆ†ç±»å™¨æ¥å¯¹å…¶è¿›è¡Œçº¿æ€§åˆ¤åˆ«ã€‚é’ˆå¯¹çº¿æ€§åˆ¤åˆ«ï¼Œæˆ‘è®¾è®¡äº†ä¸€ä¸ª `LinearClassifier` ç±»æ¥å®ç°çº¿æ€§åˆ¤åˆ«æ¨¡å‹çš„æ„å»ºã€‚

åœ¨åŸç†æ–¹é¢ï¼Œæˆ‘é‡‡ç”¨å•ä¸ªæ„ŸçŸ¥æœºçš„æ¨¡å‹è¿›è¡Œå®ç°ï¼Œè¯¥æ„ŸçŸ¥æœºçš„æ¿€æ´»å‡½æ•°è®¾è®¡ä¸º `sigmoid` å‡½æ•°ã€‚å³ï¼š$Sigmoid(x)=\frac{1}{1+e^{-x}}$ã€‚æŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µå³ï¼š$Loss(z) = ylnz+(1-y)ln(1-z)$ ã€‚è®¾è¯¥çº¿æ€§æ¨¡å‹çš„æƒé‡ä¸º $\omega$ ï¼Œåç½®ä¸º$b$ã€‚åˆ™æ•´ä½“æ¨¡å‹ä¸º $z = Sigmoid(X\omega+b)$ ã€‚è€Œæœ€ç»ˆåˆ¤åˆ«å‡½æ•°ä»¥ $0.5$ä¸ºç•Œï¼Œå¤§äºè€…ä¸ºæ­£ä¾‹ï¼Œå°äºè€…ä¸ºè´Ÿä¾‹ã€‚æ•´ä½“æŸå¤±ä¸º$Cost = \frac{1}{n}\sum Loss(z)$

æˆ‘ä»¬çš„ç›®çš„æ˜¯ä½¿å¾—æŸå¤±å€¼å°½å¯èƒ½å°ï¼Œç”±æ¢¯åº¦ä¸‹é™åˆ™
$$
d\omega = \frac{dCoss}{dz}Â·Sigmoid'(X\omega+b)Â·X^T = \frac{1}{n} X^T(z-y)
$$
åˆ™ä¸‹é™æ–¹å‘ä¸º$-d\omega = -\frac{1}{n}X^T(z-y)$

è®¾å­¦ä¹ ç‡ä¸º$\alpha$ åˆ™æ¯æ¬¡æ›´æ–°ä¸º $\omega = \omega - \alpha d\omega$

å†è®¾è®¡ä¸€ä¸ªè¿­ä»£è½®æ¬¡ï¼Œåˆ™å¯ä»¥å®ç°æ•´ä¸ªçº¿æ€§åˆ†ç±»å™¨çš„è®¾è®¡ï¼Œåœ¨å…·ä½“å®ç°çš„è¿‡ç¨‹ä¸­å°† $X$ï¼Œè½¬æ¢æˆ$[1\ \ \ X]$ï¼Œæ•´ä½“ä»£ç å¦‚ä¸‹ï¼š

$Sigmoid$ å‡½æ•°å®ç°ï¼Œä»¥åŠæ•°æ®è½¬æ¢ä»£ç ï¼š

```python
'''
PreDataProcessor.py
'''
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


def PrepareForTraining(df):
    df.insert(0,'',1)
    return df

def Sigmoid(x):
    return 1 / (1 + np.exp(-x))

```

çº¿æ€§åˆ†ç±»è®¾è®¡ï¼š

```python
'''
LinearClassifier.py
'''
import numpy as np
from DataProcessor import PrepareForTraining, Sigmoid


class LinearClassifier:
    def __init__(self, alpha=0.01, iterations=1000):
        self.alpha = alpha
        self.iterations = iterations
        self.weights = None

    def gradientDescent(self, X, y):
        n = X.shape[0]
        pred = Sigmoid(np.dot(X, self.weights))
        delta = pred - y
        self.weights -= self.alpha * (1 / n) * np.dot(X.T, delta) #äº¤å‰ç†µ $L(w) = -yln[Sigmoid(Xw)]-(1-y)ln[1-Sigmoid(Xw)]$
        # self.weights -= self.alpha * (1 / n) * np.dot(X.T, delta * pred * (1 - pred)) # $L(w)=\frac{1}{2n}[Sigmoid(Xw)-y]^2$

    def fit(self, X, y):
        X = PrepareForTraining(X)
        n, m = X.shape
        self.weights = np.zeros([m, 1])
        for _ in range(self.iterations):
            self.gradientDescent(X, y)
        return self.weights

    def predict(self, X):
        X = PrepareForTraining(X)
        return Sigmoid(np.dot(X, self.weights))>=0.5
```

éšåæˆ‘å¯¹æˆ‘è‡ªå·±çš„ä»£ç è¿›è¡Œè°ƒç”¨å’Œè®­ç»ƒï¼Œå¯ä»¥è·å¾—å¯¹åº”çš„æƒé‡

```python
from LinearClassifier import LinearClassifier
from DataProcessor import PrepareForTraining, LinearBoundary

LC = LinearClassifier(alpha=0.1, iterations=10000)
weight = LC.fit(X, y)
```

æœ€ç»ˆé€šè¿‡ç®€å•çš„æ•°æ®å¤„ç†ï¼Œåœ¨è¯¥åŒºé—´å†…ç»˜åˆ¶å‡ºçº¿æ€§åˆ†ç±»ç›´çº¿ï¼Œç»˜åˆ¶å¯¹åº”çš„åˆ†ç±»æŸ“è‰²ï¼ŒåŸºäºå¦‚ä¸‹ä»£ç å®ç°ã€‚æœ€ç»ˆåœ¨æ‰§è¡Œåå¯ä»¥è·å¾—å¦‚ä¸‹å›¾ã€‚

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.colors import ListedColormap

x_min, x_max = X['x1'].min() - 0.2, X['x1'].max() + 0.2
y_min, y_max = X['x2'].min() - 0.2, X['x2'].max() + 0.2
x = np.linspace(x_min, x_max, 100)
liny = LinearBoundary(weight, x)
plt.plot(x, liny, 'k')

custom_cmap = ListedColormap(['#9898ff',  # æµ…çº¢
                              '#FFC0CB',])

x0, x1 = np.meshgrid(np.linspace(x_min, x_max, 1000).reshape(-1, 1),
                     np.linspace(y_min, y_max, 1000).reshape(-1, 1))
X_new = np.c_[x0.ravel(), x1.ravel()]
X_new = pd.DataFrame(X_new)
Y_new = LC.predict(X_new)
z = Y_new.reshape(x0.shape)
plt.contourf(x0, x1, z, cmap=custom_cmap)

plt.axis([x_min, x_max, y_min, y_max])
plt.legend(loc='lower left')
plt.show()
```

![](G:\ExpMachineLearn\ExpML\Exp2\images\part1å®éªŒç»“æœ.png)

å³å¯å®ç°å¯¹å®éªŒè¯¥éƒ¨åˆ†çš„å®ç°ã€‚

### 2 æœ€å¤§ä¼¼ç„¶ä¼°è®¡

#### 2.1 å®éªŒå†…å®¹

æŒæ¡ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡è¿›è¡Œå‚æ•°ä¼°è®¡çš„åŸç†ï¼›å½“è®­ç»ƒæ ·æœ¬æœä»å¤šå…ƒæ­£æ€åˆ†å¸ƒæ—¶ï¼Œè®¡ç®—ä¸åŒé«˜æ–¯æƒ…å†µä¸‹çš„å‡å€¼å’Œæ–¹å·®ã€‚

ä½¿ç”¨ä¸Šé¢ç»™å‡ºçš„ä¸‰ç»´æ•°æ®æˆ–è€…ä½¿ç”¨ `exp2_2.xlsx `ä¸­çš„æ•°æ®ï¼š

(1) ç¼–å†™ç¨‹åºï¼Œå¯¹ç±» 1 å’Œç±» 2 ä¸­çš„ä¸‰ä¸ªç‰¹å¾$x_i$åˆ†åˆ«æ±‚è§£æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡å€¼$ğœ‡Ì‚$å’Œæ–¹å·®$ğœ^2$ã€‚

(2) ç¼–å†™ç¨‹åºï¼Œå¤„ç†äºŒç»´æ•°æ®çš„æƒ…å½¢$p(x) \sim N(\mu,\sum)$ã€‚å¯¹ç±» 1 å’Œç±» 2 ä¸­ä»»æ„ä¸¤ä¸ªç‰¹å¾çš„ç»„åˆåˆ†åˆ«æ±‚è§£æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡å€¼ $\hat\mu$ å’Œæ–¹å·® $\hat\sum$ï¼ˆæ¯ä¸ªç±»æœ‰3ç§å¯èƒ½ï¼‰ã€‚

(3) ç¼–å†™ç¨‹åºï¼Œå¤„ç†ä¸‰ç»´æ•°æ®çš„æƒ…å½¢$p(x) \sim N(\mu,\sum)$ã€‚å¯¹ç±» 1 å’Œç±» 2 ä¸­ä¸‰ä¸ªç‰¹å¾æ±‚è§£æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡å€¼ $\hat\mu$ å’Œæ–¹å·®$\hat\sum$ã€‚

(4) å‡è®¾è¯¥ä¸‰ç»´é«˜æ–¯æ¨¡å‹æ˜¯å¯åˆ†ç¦»çš„ï¼Œå³$\sum = diag (\sigma^2_1,\sigma^2_2,\sigma^2_3)$ï¼Œç¼–å†™ç¨‹åºä¼°è®¡ç±» 1 å’Œç±» 2 ä¸­çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µä¸­çš„å‚æ•°ã€‚

(5) æ¯”è¾ƒå‰ 4 ç§æ–¹æ³•è®¡ç®—å‡ºæ¥çš„æ¯ä¸€ä¸ªç‰¹å¾çš„å‡å€¼$\mu_i$çš„å¼‚åŒï¼Œå¹¶åŠ ä»¥è§£é‡Šã€‚

(6) æ¯”è¾ƒå‰ 4 ç§æ–¹æ³•è®¡ç®—å‡ºæ¥çš„æ¯ä¸€ä¸ªç‰¹å¾çš„æ–¹å·®$\sigma_i$çš„å¼‚åŒï¼Œå¹¶åŠ ä»¥è§£é‡Šã€‚

#### 2.2 å®éªŒè¿‡ç¨‹

é¦–å…ˆæ˜¯åŸºæœ¬çš„æ•°æ®è¯»å…¥ï¼Œç”±äºè¯¥ç»„æ•°æ®ä¸º `.xlsx` æ–‡ä»¶ï¼Œå› æ­¤é‡‡ç”¨`pandas` å¯ä»¥å¯¹å…¶è¿›è¡Œè¯»å…¥ï¼Œè¯»å…¥ååå†åšå‰©ä½™çš„å¤„ç†ï¼Œé‡‡ç”¨æœ€ç®€å•çš„`pd.read_excel` æ¥å®ç°ï¼Œå¹¶ä¸”åˆ©ç”¨ `y` å¯¹åº”çš„å€¼å¯¹å…¶è¿›è¡Œåˆ†ç±»ä»¥ä¾¿åè¾¹æ‰€æœ‰é¢˜çš„å¤„ç†ã€‚

```python
df = pd.read_excel('../../data/exp2-2.xlsx')
y = df['y']
X = df.drop('y', axis=1)
X1 = X[y == 1]
X2 = X[y == 0]
```

å¯¹äºï¼ˆ1ï¼‰ï¼Œå¯¹äºæ¯ä¸ªç±»åˆ«çš„æ¯ä¸ªç‰¹å¾ $x_i$
$$
\mu = \frac{1}{n}x_i
$$

$$
\sigma^2 = \frac{1}{n} (x_i-\mu)^2
$$

åŒæ ·åœ¨pythonçš„ `numpy` åŒ…ä¸­ï¼Œæœ‰ç€`mean` å’Œ `var` ä¸¤ä¸ªå‡½æ•°å¯ä»¥ç”¨æ¥è®¡ç®—$\mu$ å’Œ $\sigma^2$ï¼Œå¯¹æ­¤æˆ‘è®¾è®¡äº†å¦‚ä¸‹ä»£ç æ¥è¿›è¡Œè®¡ç®—å’Œæ¯”è¾ƒ

```python
def Part1(data):
    '''
        ç¼–å†™ç¨‹åºï¼Œå¯¹ç±» 1 å’Œç±» 2 ä¸­çš„ä¸‰ä¸ªç‰¹å¾ğ‘¥ğ‘–åˆ†åˆ«æ±‚è§£æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡å€¼ğœ‡Ì‚å’Œæ–¹å·®ğœÌ‚2ã€‚
        å½“å‰è¦å¤„ç†çš„å·¥ä½œå³å°†ç±»ä¸­ä¸‰ä¸ªç‰¹å¾å„è‡ªçœ‹ä½œç‹¬ç«‹ï¼Œæ±‚å…¶å‡å€¼å’Œæ–¹å·®
        å³æ­¤æ—¶ Î¼å’Œ Î£ å‡ä¸çŸ¥
    '''
    # æ‰‹åŠ¨å®ç°è®¡ç®—å‡å€¼å’Œæ–¹å·®
    data = data.to_numpy()
    n, m = data.shape
    miu = np.zeros(m)
    sigma2 = np.zeros(m)
    for x in data:
        miu += x / n
    for x in data:
        sigma2 += (x - miu) ** 2 / n
    print(f'å½“å¯¹ä¸‰ä¸ªç‰¹å¾åˆ†åˆ«æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:\nmiu=\n{miu}\nsigma2=\n{sigma2}')

    # ä¹Ÿå¯ä»¥é‡‡ç”¨pandasä¸­çš„åŒ…æ¥è®¡ç®—å‡å€¼å’Œæ–¹å·®
    miu = np.mean(data, axis=0)
    sigma2 = np.var(data, axis=0, ddof=0)
    print(f'å½“å¯¹ä¸‰ä¸ªç‰¹å¾åˆ†åˆ«åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:\nmiu=\n{miu}\nsigma2=\n{sigma2}\n')

print('ç¬¬ä¸€éƒ¨åˆ†:')
print("For Class 1:")
Part1(X1)
print("For Class 2:")
Part1(X2)
```

å¾—åˆ°çš„è¿è¡Œç»“æœå¦‚ä¸‹

```txt
ç¬¬ä¸€éƒ¨åˆ†:
For Class 1:
å½“å¯¹ä¸‰ä¸ªç‰¹å¾åˆ†åˆ«æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047 -0.911 ]
sigma2=
[0.90617729 4.20071481 4.541949  ]
å½“å¯¹ä¸‰ä¸ªç‰¹å¾åˆ†åˆ«åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047 -0.911 ]
sigma2=
[0.90617729 4.20071481 4.541949  ]

For Class 2:
å½“å¯¹ä¸‰ä¸ªç‰¹å¾åˆ†åˆ«æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.4299   0.00372]
sigma2=
[0.05820804 0.04597009 0.00726551]
å½“å¯¹ä¸‰ä¸ªç‰¹å¾åˆ†åˆ«åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.4299   0.00372]
sigma2=
[0.05820804 0.04597009 0.00726551]
```

é€šè¿‡æ¯”å¯¹å‘ç°ï¼Œä¸¤è€…çš„è®¡ç®—å®Œå…¨ä¸€è‡´ã€‚

å¯¹äº (2) çš„éƒ¨åˆ†ï¼Œæˆ‘ä¾æ¬¡æšä¸¾ä¸¤ç§ç»„åˆï¼Œå¹¶ä»¥æ­¤è®¡ç®—å…¶ç»„åˆåå„è‡ªçš„$\mu $ å’Œ $\sum$ã€‚

å¯¹äºä¸€ä¸ªä¸¤ä¸¤ç»„åˆçš„æ•°æ®ï¼Œå…¶ $\mu$ çš„æ±‚æ³•ä¸ä¸Šè¿°åŸºæœ¬ä¸€è‡´ï¼Œåªä¸è¿‡æ¢æˆäº†çŸ©é˜µè¿ç®—ã€‚è€Œå¯¹äº$\sum = \frac{1}{n}\sum(x_i-\mu)Â·(x_i-\mu)^T$ã€‚å¹¶ä¸”ä¹Ÿå°è¯•é‡‡ç”¨pythonä¸­çš„`mean`æ–¹æ³•å’Œ`cov`æ–¹æ³•æ¥è¿›è¡Œæ±‚è§£ã€‚ä»£ç å’Œè¾“å‡ºå¦‚ä¸‹

```python
def DealForMatrix(data):
    data = data.to_numpy()
    n, m = data.shape
    miu = np.zeros(m)
    sigma2 = np.zeros([m, m])
    for x in data:
        miu += x
    miu = miu / n
    for x in data:
        delta = (x - miu).reshape(-1, 1)
        sigma2 += np.dot(delta, delta.T)
    sigma2 = sigma2 / n
    print(f'å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:\nmiu=\n{miu}\nsigma2=\n{sigma2}')

    # ä¹Ÿå¯ä»¥é‡‡ç”¨pandasä¸­çš„åŒ…æ¥è®¡ç®—å‡å€¼å’Œæ–¹å·®
    miu = np.mean(data, axis=0)
    sigma2 = np.cov(data, rowvar=False, ddof=0)
    print(f'å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:\nmiu=\n{miu}\nsigma2=\n{sigma2}\n')


def Part2(data):
    n, m = data.shape
    for i in range(m):
        for j in range(i + 1, m):
            print(f'é‡‡ç”¨x{i + 1}å’Œx{j + 1}è¿›è¡Œè®¡ç®—æ‰€å¾—:')
            tmp = pd.concat([data.iloc[:, i], data.iloc[:, j]], axis=1).copy()
            DealForMatrix(tmp)
            
print("ç¬¬äºŒéƒ¨åˆ†:")
print("For Class 1:")
Part2(X1)
print("For Class 2:")
Part2(X2)
```

```txt
ç¬¬äºŒéƒ¨åˆ†:
For Class 1:
é‡‡ç”¨x1å’Œx2è¿›è¡Œè®¡ç®—æ‰€å¾—:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047]
sigma2=
[[0.90617729 0.56778177]
 [0.56778177 4.20071481]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047]
sigma2=
[[0.90617729 0.56778177]
 [0.56778177 4.20071481]]

é‡‡ç”¨x1å’Œx3è¿›è¡Œè®¡ç®—æ‰€å¾—:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.911 ]
sigma2=
[[0.90617729 0.3940801 ]
 [0.3940801  4.541949  ]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.911 ]
sigma2=
[[0.90617729 0.3940801 ]
 [0.3940801  4.541949  ]]

é‡‡ç”¨x2å’Œx3è¿›è¡Œè®¡ç®—æ‰€å¾—:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.6047 -0.911 ]
sigma2=
[[4.20071481 0.7337023 ]
 [0.7337023  4.541949  ]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.6047 -0.911 ]
sigma2=
[[4.20071481 0.7337023 ]
 [0.7337023  4.541949  ]]

For Class 2:
é‡‡ç”¨x1å’Œx2è¿›è¡Œè®¡ç®—æ‰€å¾—:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216  0.4299]
sigma2=
[[ 0.05820804 -0.01321216]
 [-0.01321216  0.04597009]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216  0.4299]
sigma2=
[[ 0.05820804 -0.01321216]
 [-0.01321216  0.04597009]]

é‡‡ç”¨x1å’Œx3è¿›è¡Œè®¡ç®—æ‰€å¾—:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.00372]
sigma2=
[[ 0.05820804 -0.00478645]
 [-0.00478645  0.00726551]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.00372]
sigma2=
[[ 0.05820804 -0.00478645]
 [-0.00478645  0.00726551]]

é‡‡ç”¨x2å’Œx3è¿›è¡Œè®¡ç®—æ‰€å¾—:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[0.4299  0.00372]
sigma2=
[[0.04597009 0.00850987]
 [0.00850987 0.00726551]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[0.4299  0.00372]
sigma2=
[[0.04597009 0.00850987]
 [0.00850987 0.00726551]]
```

å¯ä»¥å‘ç°å…¶è®¡ç®—ç»“æœä¿æŒä¸€è‡´ï¼Œå³ä»£ç è®¾è®¡æ­£ç¡®ã€‚

å¯¹äº (3) éƒ¨åˆ†ï¼Œå…¶å…³é”®æ­¥éª¤äº (2) ä¿æŒä¸€è‡´ï¼Œç›´æ¥è°ƒç”¨`DealForMatrix` å³å¯ï¼Œå…¶å…·ä½“ä»£ç å’Œç»“æœå¦‚ä¸‹ï¼š

```python
def Part3(data):
    DealForMatrix(data)
    
print("ç¬¬ä¸‰éƒ¨åˆ†:")
print("For Class 1:")
Part3(X1)
print("For Class 2:")
Part3(X2)
```

```txt
ç¬¬ä¸‰éƒ¨åˆ†:
For Class 1:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047 -0.911 ]
sigma2=
[[0.90617729 0.56778177 0.3940801 ]
 [0.56778177 4.20071481 0.7337023 ]
 [0.3940801  0.7337023  4.541949  ]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047 -0.911 ]
sigma2=
[[0.90617729 0.56778177 0.3940801 ]
 [0.56778177 4.20071481 0.7337023 ]
 [0.3940801  0.7337023  4.541949  ]]

For Class 2:
å½“æ‰‹ç®—æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.4299   0.00372]
sigma2=
[[ 0.05820804 -0.01321216 -0.00478645]
 [-0.01321216  0.04597009  0.00850987]
 [-0.00478645  0.00850987  0.00726551]]
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.4299   0.00372]
sigma2=
[[ 0.05820804 -0.01321216 -0.00478645]
 [-0.01321216  0.04597009  0.00850987]
 [-0.00478645  0.00850987  0.00726551]]
```

å¯¹äºç¬¬å››éƒ¨åˆ†ï¼Œå½“$\sum = diag (\sigma^2_1,\sigma^2_2,\sigma^2_3)$ æ—¶ï¼Œå…¶å¯¹åº”çš„ $\sigma$ å’Œç¬¬ä¸€éƒ¨åˆ†è®¡ç®—ä¿æŒä¸€è‡´ï¼Œå› æ­¤ä¸å†è¿‡å¤šèµ˜è¿°ã€‚

```python
def Part4(data):
    '''
    ç»å¤§å¤šæ•°ä»£ç åœ¨Part1ä¸­å·²ç»å‘ˆç°ï¼Œå› æ­¤æ­¤å¤„ç›´æ¥ä½¿ç”¨numpyå®ç°
    '''
    data = data.to_numpy()
    miu = np.mean(data,axis=0)
    sigma2 = np.var(data,axis=0,ddof=0)
    cov = np.diag(sigma2)
    print(f'å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:\nmiu=\n{miu}\ncov=\n{cov}\n')

print("ç¬¬å››éƒ¨åˆ†:")
print("For Class 1:")
Part4(X1)

print("For Class 2:")
Part4(X2)
```

```
ç¬¬å››éƒ¨åˆ†:
For Class 1:
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.0709 -0.6047 -0.911 ]
cov=
[[0.90617729 0.         0.        ]
 [0.         4.20071481 0.        ]
 [0.         0.         4.541949  ]]

For Class 2:
å½“åˆ©ç”¨numpyæ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ—¶:
miu=
[-0.1216   0.4299   0.00372]
cov=
[[0.05820804 0.         0.        ]
 [0.         0.04597009 0.        ]
 [0.         0.         0.00726551]]
```

å¯¹äº (5) å’Œ(6)åœ¨æ±‚è§£å‡å€¼å’Œåæ–¹å·®çš„è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œåæ–¹å·®è®¡ç®—ä¸­é™¤ä»¥`n`æ˜¯åˆé€‚çš„ã€‚å³é‡‡ç”¨æœ‰åä¼°è®¡ã€‚æ•´ä½“çš„å€¼å‡ä¿æŒä¸€è‡´ã€‚

- å¯¹æ¯”å‡å€¼ï¼šå„ç§æ–¹æ³•è®¡ç®—çš„å‡å€¼åº”è¯¥ä¸€è‡´ï¼Œå› ä¸ºå‡å€¼çš„ä¼°è®¡ä¸åæ–¹å·®çŸ©é˜µçš„å½¢å¼æ— å…³ã€‚
- å¯¹æ¯”æ–¹å·®ï¼šç›´æ¥æ–¹å·®ä¼°è®¡ä¸åœ¨åæ–¹å·®çŸ©é˜µä¸­æå–çš„æ–¹å·®åº”ä¸€è‡´ã€‚ç„¶è€Œï¼Œå¯¹è§’åæ–¹å·®çŸ©é˜µåªè€ƒè™‘å•ä¸ªå˜é‡çš„æ–¹å·®ï¼Œå¿½ç•¥å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¯èƒ½åœ¨æŸäº›åº”ç”¨ä¸­æä¾›ä¸åŒçš„è§†è§’ã€‚

### 3-Parzençª—

#### 3-Parzen.1 å®éªŒå†…å®¹

ä½¿ç”¨ä¸Šé¢è¡¨æ ¼ä¸­çš„æ•°æ®æˆ–è€…ä½¿ç”¨ exp2_3.xlsx ä¸­çš„æ•°æ®è¿›è¡Œ Parzen çª—ä¼°è®¡å’Œè®¾è®¡åˆ†ç±»å™¨ã€‚çª—å‡½æ•°ä¸ºä¸€ä¸ªçƒå½¢çš„é«˜æ–¯å‡½æ•°å¦‚å…¬å¼2-1æ‰€ç¤ºï¼š
$$
\varphi(\frac{x-x_i}{h}) \varpropto exp[-\frac{(x-x_i)^T(x-x_i)}{2h^2}]
$$
ç¼–å†™ç¨‹åºï¼Œä½¿ç”¨ Parzen çª—ä¼°è®¡æ–¹æ³•å¯¹ä»»æ„ä¸€ä¸ªçš„æµ‹è¯•æ ·æœ¬ç‚¹ğ‘¥ğ‘¥è¿›è¡Œåˆ†ç±»ã€‚å¯¹åˆ†ç±»å™¨çš„è®­ç»ƒåˆ™ä½¿ç”¨è¡¨2-2ä¸­çš„ä¸‰ç»´æ•°æ®ã€‚ä»¤$h = 1$ï¼Œåˆ†ç±»æ ·æœ¬ç‚¹ä¸º$(0.5,1.0,0.0)^T$ï¼Œ$(0.31,1.51,-0.50)^T$ï¼Œ$(-0.3,0.44, -0.1)^T$ ã€‚

#### 3-Parzen.2 å®éªŒè¿‡ç¨‹

Parzençª—çš„åŸºæœ¬åŸç†ä¸ºä»¥å½“å‰ç‚¹ä¸ºä¸­å¿ƒï¼Œç»˜åˆ¶ä¸€ä¸ªhçš„é«˜æ–¯çª—å£ï¼Œå¯¹å…¶ä¸­è¿›è¡Œæ•°ç‚¹ï¼Œå³å¯è¿”å›ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦ï¼Œåœ¨ç†æƒ³æƒ…å†µä¸‹ä¹Ÿå¯æ ¹æ®ç‚¹æ•°çš„å¤šå°‘ç›´æ¥è¿›è¡Œåˆ†ç±»ã€‚é‚£ä¹ˆåŸºäºå¦‚ä¸ŠåŸºç¡€è¿›è¡Œäº†ç®€å•ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


def ParzenWindow(X, h, test):
    X = X.to_numpy()
    sigma = 0
    for x in X:
        sigma += np.exp(-np.dot((test - x), (test - x).T) / 2 * h ** 2)
    return sigma


def Parzen(X, y, h, test):
    test = np.array(test)
    yList = y.drop_duplicates().tolist()
    res = np.zeros([len(yList), 1])
    for index,testX in enumerate(test):
        K = np.zeros(len(yList))
        for i, yEle in enumerate(yList):
            XList = X[y == yEle]
            k = ParzenWindow(XList, h, testX)
            K[i] = k / (XList.shape[0] * h ** XList.shape[1])
        res[index] = yList[np.argmax(K)]
    return res


df = pd.read_excel('../../data/exp2-3.xlsx')
df.to_csv('../../data/exp2-3.csv', index=False)
y = df['y']
X = df.drop('y', axis=1)
Xtest = [[0.5, 1.0, 0.0],
         [0.31, 1.51, -0.50],
         [-0.3, 0.44, -0.1]]
yPred = Parzen(X, y, 1, Xtest)
print(yPred)
```

åœ¨è¿›è¡Œè®¡ç®—åï¼Œå¯ä»¥è·å¾—å¦‚ä¸‹ç»“æœï¼š

```txt
[[2.]
 [2.]
 [2.]]
```

åˆ™å°†è¯¥è¿‡ç¨‹å®ç°ã€‚

### 3-KNN

#### 3-KNN.1 å®éªŒå†…å®¹

k-è¿‘é‚»æ¦‚ç‡å¯†åº¦ä¼°è®¡ï¼š

å¯¹ä¸Šé¢è¡¨æ ¼ä¸­çš„æ•°æ®ä½¿ç”¨k-è¿‘é‚»æ–¹æ³•è¿›è¡Œæ¦‚ç‡å¯†åº¦ä¼°è®¡ï¼š

1) ç¼–å†™ç¨‹åºï¼Œå¯¹äºä¸€ç»´çš„æƒ…å†µï¼Œå½“æœ‰$ n $ä¸ªæ•°æ®æ ·æœ¬ç‚¹æ—¶ï¼Œè¿›è¡Œ$k$-è¿‘é‚»æ¦‚ç‡å¯†åº¦ä¼°è®¡ã€‚å¯¹è¡¨æ ¼ä¸­çš„ç±»$3$çš„ç‰¹å¾$x_1$ï¼Œç”¨ç¨‹åºç”»å‡ºå½“$ k=1,3,5$ æ—¶çš„æ¦‚ç‡å¯†åº¦ä¼°è®¡ç»“æœã€‚
2) ç¼–å†™ç¨‹åºï¼Œå¯¹äºäºŒç»´çš„æƒ…å†µï¼Œå½“æœ‰ n ä¸ªæ•°æ®æ ·æœ¬ç‚¹æ—¶ï¼Œè¿›è¡Œk-è¿‘é‚»æ¦‚ç‡å¯†åº¦ä¼°è®¡ã€‚å¯¹è¡¨æ ¼ä¸­çš„ç±»$2$çš„ç‰¹å¾$(x_1, x_2)^T$ï¼Œç”¨ç¨‹åºç”»å‡ºå½“ $k=1,3,5 $æ—¶çš„æ¦‚ç‡å¯†åº¦ä¼°è®¡ç»“æœã€‚
3) ç¼–å†™ç¨‹åºï¼Œå¯¹è¡¨æ ¼ä¸­çš„$3$ä¸ªç±»åˆ«çš„ä¸‰ç»´ç‰¹å¾ï¼Œä½¿ç”¨$k$-è¿‘é‚»æ¦‚ç‡å¯†åº¦ä¼°è®¡æ–¹æ³•ã€‚å¹¶ä¸”å¯¹ä¸‹åˆ—ç‚¹å¤„çš„æ¦‚ç‡å¯†åº¦è¿›è¡Œä¼°è®¡ï¼š$(-0.41,0.82,0.88)^Tï¼Œ(0.14,0.72, 4.1)^Tï¼Œ(-0.81,0.61, -0.38)^T$ã€‚

#### 3-KNN.2 å®éªŒè¿‡ç¨‹

å¯¹äºæ•´ä¸ªå®éªŒæˆ‘å¤§è‡´æœ‰ä¸¤ç§ä¸åŒçš„ç†è§£æ–¹æ¡ˆï¼Œå…¶å®é™…çš„é¢„æµ‹ç»“æœå·®å€¼ä¸å¤§ï¼Œä½†å…¶å®é™…è¿ç®—çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦å…·æœ‰ä¸€å®šçš„å·®å¼‚æ€§ï¼Œè€Œä¸”åœ¨åæ®µçš„é¢„æµ‹è¿‡ç¨‹ä¸­ä½¿ç”¨äº†è´å¶æ–¯çš„æ€æƒ³ï¼Œåœ¨é¢„æµ‹ä¸ä»…ä»…åªè€ƒè™‘ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦ï¼Œä¹Ÿè€ƒè™‘è¿›å…ˆéªŒæ¦‚ç‡ã€‚

æˆ‘çš„ç¬¬ä¸€ç§ç†è§£æ—¶ä»…è€ƒè™‘é¢˜é¢æ‰€æåŠçš„ç±»3ï¼ˆé¢˜1ï¼‰ï¼Œå’Œç±»2ï¼ˆé¢˜2ï¼‰ã€‚ä»…ä½¿ç”¨è¿™ä¸€ä¸ªç±»ï¼Œåˆ©ç”¨knnæ±‚å¾—è¿™ä¸€ç±»å¾—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦ï¼ˆä¸å…¶ä»–ç±»æ— å…³ï¼‰ã€‚å³kä¸ªç‚¹åªé’ˆå¯¹ä¸€ä¸ªæ ·æœ¬ï¼Œè€ƒè™‘åˆ°å¦‚æœè¿™ä¸ªç‚¹é™„è¿‘çš„ä¸å¤Ÿå¯†ï¼Œé‚£ä¹ˆå…¶è·ç¦»ä¼šå¾ˆè¿œï¼Œåˆ™ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦å¾ˆå°ã€‚

ä½¿ç”¨å¤šä¸ªç±»ï¼Œä½†æ˜¯åªå±•ç¤ºç±»3ï¼ˆé¢˜1ï¼‰ï¼Œç±»2ï¼ˆé¢˜2ï¼‰çš„æ¦‚ç‡å¯†åº¦ã€‚å³kä¸ªç‚¹æ˜¯æ‰€æœ‰æ ·æœ¬ã€‚é‚£ä¹ˆåœ¨è¿™kä¸ªç‚¹ä¸­æœ€å¤šçš„é‚£ç±»çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦åº”å½“æœ€é«˜ã€‚

å†åˆ©ç”¨æ±‚å¾—çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦ä¸å…ˆéªŒæ¦‚ç‡ç›¸ä¹˜å†æ¯”è¾ƒå¤§å°è¿›è¡Œé¢„æµ‹ã€‚

åœ¨è®¡ç®—å…¶ä¸­çš„è·ç¦»åå¯¹åº”çš„ä½“ç§¯æ—¶ï¼Œæˆ‘é‡‡ç”¨äº†é«˜ç»´çƒä½“çš„ä½“ç§¯è®¡ç®—å…¬å¼è¿›è¡Œ

è¶…çƒä½“ä½“ç§¯ $V_n(r) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} r^n$ ï¼Œä»¤ $ \alpha = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} $ä¸ºç³»æ•°ï¼Œåˆ™ $V_n(r) = \alpha r^n$ 

å¹¶ä¸”åŸºäºæˆ‘ä¸ªäººå¯¹äºKNNçš„ç†è§£ï¼Œæ„å»ºäº†ä¸€ä¸ªKNNç±»æ¥å®ç°è¯¥å®éªŒä»¥åŠ4 KNNå®æˆ˜çš„éƒ¨åˆ†çš„KNNä»£ç ï¼Œå¦‚ä¸‹ï¼š

```python
'''
è¯¥ä»£ç ä¸ºåœ¨å®éªŒ4å†…å®¹ä¿®æ”¹åçš„knnä»£ç 
'''

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from numpy.linalg import inv
import math


class kNNClassifier:

    def KNN_Vn(self, X, k, t):
        '''
        åŸºäº p_{n}(x) = k_{n}/(V_{n}*n) æ¥è®¡ç®—æ¦‚ç‡å¯†åº¦
        :param X: åŸå§‹æ•°æ®é›†çš„æŸä¸€ç±»åˆ«
        :param k: KNN çš„k
        :param t: æµ‹è¯•é›†
        :return: ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°
        '''
        res = np.zeros((t.shape[0],))
        for i, test in enumerate(t):
            dist = np.linalg.norm(X - test, axis=1)
            dist = np.sort(dist)
            if dist[k - 1] != 0:
                res[i] = k / (X.shape[0] * dist[k - 1] ** X.shape[1])
            else:
                res[i] = k / (X.shape[0] * (1e-5) ** X.shape[1])
        return res

    def kNN_Vn(self, X_train, y_train, X_test, k):
        res = [{} for _ in range(X_test.shape[0])]
        yValueList = y_train.unique().tolist()
        for y in yValueList:
            trainSet = X_train[y_train == y].copy()
            yRes = self.KNN_Vn(trainSet, k, X_test)
            for i in range(len(yRes)):
                # ä¸º res ä¸­çš„æ¯ä¸€è¡Œçš„å­—å…¸æ·»åŠ é”®å€¼å¯¹ï¼Œé”®ä¸º yï¼Œå€¼ä¸º yRes ä¸­å¯¹åº”çš„å…ƒç´ 
                res[i][y] = yRes[i]
        return res

    def gamma(self, x):
        # é€’å½’åˆ©ç”¨å·²çŸ¥çš„Gamma(1/2) = sqrt(pi)
        if abs(x - 0.5) < 1e-6:
            return math.sqrt(math.pi)
        elif abs(x - 1) < 1e-6:
            return 1
        else:
            return (x - 1) * self.gamma(x - 1)

    def kNN_Euler_Count(self, X_train, y_train, X_test, k):
        '''
        åŸºäºæ¬§æ‹‰è·ç¦»çš„è®¡æ•°æ–¹æ³•æ¥è®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        :param X_train: è®­ç»ƒé›†
        :param y_train: è®­ç»ƒæ ‡ç­¾
        :param X_test: æµ‹è¯•é›†
        :param k: kè¿‘é‚»çš„k
        :return: æµ‹è¯•é›†çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        '''
        res = []
        # è¶…çƒä½“ä½“ç§¯ç³»æ•° $V_n(r) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} r^n$ ä»¤ $ alpha = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} $
        pi_power = math.pi ** (X_train.shape[1] / 2)
        gamma_value = self.gamma(((X_train.shape[1] / 2) + 1))
        alpha = ((pi_power) / gamma_value) if gamma_value != 0 else float('inf')
        for i, test in enumerate(X_test):
            dist = np.linalg.norm(X_train - test, axis=1)
            KNN_indices = np.argsort(dist)[:k]
            KNN_labels = y_train[KNN_indices]
            d = dist[KNN_indices[-1]]
            if d == 0:
                d = 1e-5
            class_probs = {
                cls: np.sum(KNN_labels == cls) / (X_train.shape[0] * alpha * (d ** X_train.shape[1]))
                for cls in np.unique(y_train)}
            res.append(class_probs)
        return res

    def mahalanobis_distance(self, x, dataset, invCov):
        '''
        è®¡ç®—å•ä¸ªæ ·æœ¬ä¸æ•°æ®é›†æ‰€æœ‰æ ·æœ¬çš„é©¬æ°è·ç¦»
        :param x:
        :param dataset:
        :param invCov:
        :return:
        '''
        dist = np.zeros(dataset.shape[0])
        dataset = dataset.to_numpy()
        for i, data in enumerate(dataset):
            delta = x - data
            dist[i] = np.sqrt(delta.dot(invCov).dot(delta.T))
        return dist

    def kNN_Mahalanobis_Count(self, X_train, y_train, X_test, k):
        '''
        åŸºäºé©¬æ°è·ç¦»çš„è®¡æ•°æ–¹æ³•æ¥è®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        :param X_train: è®­ç»ƒé›†
        :param y_train: è®­ç»ƒæ ‡ç­¾
        :param X_test: æµ‹è¯•é›†
        :param k: kè¿‘é‚»çš„k
        :return: æµ‹è¯•é›†çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        '''
        # è¶…çƒä½“ä½“ç§¯ç³»æ•° $V_n(r) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} r^n$ ä»¤ $ alpha = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} $
        pi_power = math.pi ** (X_train.shape[1] / 2)
        gamma_value = self.gamma((X_train.shape[1] / 2) + 1)
        alpha = ((pi_power) / gamma_value) if gamma_value != 0 else float('inf')
        cov = np.cov(X_train.T)
        invCov = inv(cov)
        res = []
        for i, test in enumerate(X_test):
            dist = self.mahalanobis_distance(test, X_train, invCov)
            KNN_indices = np.argsort(dist)[:k]
            KNN_labels = y_train[KNN_indices]
            d = dist[KNN_indices[-1]]
            if d == 0:
                d = 1e-5
            class_probs = {
                cls: np.sum(KNN_labels == cls) / (X_train.shape[0] * alpha * (d ** X_train.shape[1]))
                for cls in np.unique(y_train)}
            res.append(class_probs)
        return res

    def density(self, X_train, y_train, X_test, k, typ=0):
        yValueList = y_train.unique().tolist()
        prior_prob = {}
        if not isinstance(X_test, np.ndarray):
            # å¦‚æœä¸æ˜¯ï¼Œè½¬æ¢å®ƒä¸ºnumpyæ•°ç»„
            X_test = np.array(X_test)
        for y in yValueList:
            prior_prob[y] = X_train[y_train == y].shape[0]
        if typ == 0:
            probs = self.kNN_Euler_Count(X_train, y_train, X_test, k)
        elif typ == 1:
            probs = self.kNN_Mahalanobis_Count(X_train, y_train, X_test, k)
        elif typ == 2:
            probs = self.kNN_Vn(X_train, y_train, X_test, k)
        return probs
```

å¦‚ä¸Šè¿°å®éªŒä¸€è‡´ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå¯¹æ•°æ®è¿›è¡Œè¯»å…¥ï¼Œä¾æ—§é‡‡ç”¨`pandas` åŒ…ä¸­çš„ `read_excel` æ–¹æ³•å°†å…¶è¯»å…¥ä¸ºä¸€ä¸ª`dataFrame`ï¼Œå¹¶å°† `X` å’Œ `y` æå–å‡ºæ¥ï¼Œå¹¶ä¸”å¯¼å…¥(3)ä¸­éœ€è¦é¢„æµ‹çš„æ•°æ®

```python
df = pd.read_excel('../../data/exp2-3.xlsx')
df.to_csv('../../data/exp2-3.csv', index=False)
y = df['y']
X = df.drop('y', axis=1)
Xtest = [[-0.41, 0.82, 0.88],
         [0.14, 0.72, 4.1],
         [-0.81, 0.61, -0.38]]
Xtest = np.array(Xtest)
```

ä»¥ä¸‹ä¸ºä¸¤ç§ä¸åŒçš„ç†è§£å¯¹åº”çš„å„è‡ªç»“æœå’Œå›¾åƒã€‚

é¦–å…ˆå¯¹äº(1)ï¼š

å¯¹äºç¬¬ä¸€ç§ç†è§£ï¼Œéœ€è¦å°†æ•°æ®æ ¹æ®ç±»å®Œå…¨æ‘˜ç¦»ï¼Œæ•°æ®å¤„ç†å³ä¸ºï¼š

```python
X1 = X['x1'].copy()
X_train = X1[y == 3].copy().reset_index(drop=True)
y_train = y[y == 3].copy().reset_index(drop=True)
DealPart1(X_train, y_train)
```

å¯¹äºç¬¬äºŒç§ç†è§£ï¼Œä¸éœ€è¦å•ç‹¬æ‘˜å‡ºæ•°æ®ï¼Œæ•°æ®é¢„å¤„ç†ä¸ºï¼š

```python
X1 = X['x1'].copy()
X_train = X1.copy().reset_index(drop=True)
y_train = y.copy().reset_index(drop=True)
DealPart1(X_train, y_train)
```

è€Œä¸¤è€…è°ƒç”¨KNNçš„æ–¹æ³•ä¿æŒä¸€è‡´ï¼Œå³å¦‚ä¸‹å‡½æ•°`DealPart1`ï¼š

```python
def DealPart1(X_train, y_train):
    x_min = X_train.min()
    x_max = X_train.max()
    X_train = X_train.to_numpy().reshape(-1, 1)
    X_test = np.linspace(x_min, x_max, 1000, endpoint=True).reshape(-1, 1)
    knn = kNNClassifier()

    # è·å¾—æ¦‚ç‡å¯†åº¦å‡½æ•°
    y1_density = knn.density(X_train, y_train, X_test, 1)
    y3_density = knn.density(X_train, y_train, X_test, 3)
    y5_density = knn.density(X_train, y_train, X_test, 5)

    y1_test = np.array([d[3] for d in y1_density]).reshape(-1, 1)
    y3_test = np.array([d[3] for d in y3_density]).reshape(-1, 1)
    y5_test = np.array([d[3] for d in y5_density]).reshape(-1, 1)

    plt.figure(figsize=(16, 9))
    plt.subplot(131)
    plt.plot(X_test, y1_test, 'r')
    plt.xlim([x_min, x_max])
    plt.title('k=1')

    plt.subplot(132)
    plt.plot(X_test, y3_test, 'b')
    plt.xlim([x_min, x_max])
    plt.title('k=3')

    plt.subplot(133)
    plt.plot(X_test, y5_test, 'g')
    plt.xlim([x_min, x_max])
    plt.title('k=5')

    plt.suptitle('Part1')
    plt.show()
```

åˆ†åˆ«æ‰§è¡Œåï¼š

å¯¹äºç¬¬ä¸€ç§ç†è§£ä¸åŒkä¸‹çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦å¦‚ä¸‹ï¼š

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-1ç†è§£1.png)

å¯¹äºç¬¬äºŒç§ç†è§£ä¸åŒkä¸‹çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦å¦‚ä¸‹ï¼š

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-1ç†è§£2.png)

å¦‚æœå°†å…¶å¯¹åº”çš„xè½´å¯¹å…¶ï¼Œå¯ä»¥å‘ç°ç†è§£1çš„å›¾åƒæ›´åŠ å…‰æ»‘ï¼Œå› ä¸ºå…¶åœ¨æ•°ç‚¹çš„è¿‡ç¨‹ä¸­æ–°å¢ä¸€ä¸ªç‚¹å¢åŠ è·ç¦»å‡ä¿æŒä¸€å®šçš„ä¸€è‡´æ€§ï¼Œå› æ­¤æ•´ä½“å›¾åƒæ¯”è¾ƒå…‰æ»‘ã€‚è€Œå¯¹äºç¬¬äºŒç§ç†è§£çš„æƒ…å†µä¸‹ç”±äºè€ƒè™‘åˆ°kä¸ªç‚¹çš„èŒƒå›´å†…ä¸ä¸€å®šå­˜åœ¨è¯¥ç±»çš„ç‚¹ï¼Œå› æ­¤ä¼šå‡ºç°æ•°æ®ä¸º0çš„æƒ…å†µã€‚

è€Œå¯¹äºk=1çš„æ—¶å€™ï¼Œç”±äºä¼šå¯¼è‡´è·ç¦»å˜ä¸º0ï¼Œå› æ­¤éœ€è¦è¿›è¡Œè®¾ç½®ä¸€ä¸ªæå°çš„æ•°å€¼æ¥ä¿è¯å…¶æ•°æ®è¾“å‡ºçš„åˆç†æ€§ã€‚

åŒæ ·ç›¸ä»¿ä¸Šè¿°å¯¹äº(1)çš„æ“ä½œï¼Œå¯¹äº(2)çš„æ“ä½œæˆ‘ä»¬ä»éœ€å…ˆè¿›è¡Œæ•°æ®å¤„ç†æ˜¯å¦æ‘˜é™¤ï¼Œå†æ‰§è¡ŒKNNçš„æ“ä½œã€‚

å¯¹äºç†è§£1çš„æ•°æ®å¤„ç†å¦‚ä¸‹ï¼š

```python
X2 = pd.concat([X['x1'], X['x2']], axis=1)
X_train = X2[y == 2].copy().reset_index(drop=True)
y_train = y[y == 2].copy().reset_index(drop=True)
DealPart2(X_train, y_train)
```

å¯¹äºç†è§£2çš„æ•°æ®å¤„ç†å¦‚ä¸‹ï¼š

```python
X2 = pd.concat([X['x1'], X['x2']], axis=1)
X_train = X2.copy().reset_index(drop=True)
y_train = y.copy().reset_index(drop=True)
DealPart2(X_train, y_train)
```

åŒæ ·ä¸¤è€…è®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦çš„å‡½æ•°ç›¸åŒï¼Œä¸º `DealPart2` å¦‚ä¸‹

```python
def DrawPart2(x0, x1, z, k):
    # matplotlib.use('TkAgg')
    # åˆ›å»ºä¸€ä¸ªå›¾å½¢å’Œä¸¤ä¸ªå­å›¾ï¼ˆä¸€ä¸ª2Dï¼Œä¸€ä¸ª3Dï¼‰
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

    # ç¬¬ä¸€ä¸ªå­å›¾ä¸º3Då›¾
    ax1 = fig.add_subplot(121, projection='3d')
    surf = ax1.plot_surface(x0, x1, z, cmap='viridis')
    ax1.set_xlabel('X1')
    ax1.set_ylabel('X2')
    ax1.set_zlabel('p')
    ax1.set_title(f'k={k}, 3D')
    ax1.patch.set_visible(False)
    ax1.grid(False)

    # ç¬¬äºŒä¸ªå­å›¾ä¸º2Dç­‰é«˜çº¿å›¾
    contour = ax2.contourf(x0, x1, z, cmap='viridis')
    ax2.set_xlabel('x1')
    ax2.set_ylabel('x2')
    ax2.set_title(f'k={k}, 2D')
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    # æ˜¾ç¤ºå›¾å½¢
    plt.show()

def DealPart2(X_train, y_train):
    x1_min = X_train['x1'].min()
    x1_max = X_train['x1'].max()
    x2_min = X_train['x2'].min()
    x2_max = X_train['x2'].max()
    x0, x1 = np.meshgrid(np.linspace(x1_min, x1_max, 100).reshape(-1, 1),
                         np.linspace(x2_min, x2_max, 100).reshape(-1, 1))
    X_test = np.c_[x0.ravel(), x1.ravel()]
    X_train = X_train.to_numpy()
    knn = kNNClassifier()
    y1_post = knn.density(X_train, y_train, X_test, 1)
    y3_post = knn.density(X_train, y_train, X_test, 3)
    y5_post = knn.density(X_train, y_train, X_test, 5)

    # è·å¾—æ¦‚ç‡å¯†åº¦å‡½æ•°
    y1_density = knn.density(X_train, y_train, X_test, 1)

    y3_density = knn.density(X_train, y_train, X_test, 3)
    y5_density = knn.density(X_train, y_train, X_test, 5)

    y1_test = np.array([d[2] for d in y1_density]).reshape(-1, 1)
    y3_test = np.array([d[2] for d in y3_density]).reshape(-1, 1)
    y5_test = np.array([d[2] for d in y5_density]).reshape(-1, 1)

    DrawPart2(x0, x1, y1_test.reshape(x0.shape), 1)
    DrawPart2(x0, x1, y3_test.reshape(x0.shape), 3)
    DrawPart2(x0, x1, y5_test.reshape(x0.shape), 5)
```

å…¶è¿è¡Œçš„ç»“æœå¦‚ä¸‹ï¼š

å·¦ä¾§ä¸ºäºŒç»´æ›²é¢å›¾ï¼Œå³ä¾§ä¸ºå¯¹åº”çš„ç­‰é«˜çº¿

ç¬¬ä¸€ç§ç†è§£ï¼š

k=1

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-2ç†è§£1k=1.png)

k=3

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-2ç†è§£1k=3.png)

k=5

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-2ç†è§£1k=5.png)

ç¬¬äºŒç§ç†è§£ï¼š

k=1

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-2ç†è§£2k=1.png)

k=3

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-2ç†è§£2k=3.png)

k=5

![](G:\ExpMachineLearn\ExpML\Exp2\images\part3-2ç†è§£2k=5.png)

è€Œå¯¹äº(3) é‡‡ç”¨ä¸åŒçš„kè¿›è¡Œæ‰§è¡Œï¼Œè·å¾—å…¶å¯¹åº”çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦å³å¯ï¼Œæ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼Œå¾—åˆ°å¦‚ä¸‹ç»“æœ

```python
knn = kNNClassifier()
for k in range(5):
    density = knn.density(X, y, Xtest, k + 1,2)
    print(f'k={k + 1} æ—¶çš„æ¦‚ç‡å¯†åº¦')
    for index in range(len(density)):
        print(density[index])
```

```txt
k=1 æ—¶çš„æ¦‚ç‡å¯†åº¦
{1: 0.01883296229251353, 2: 0.22167283203251173, 3: 0.07712096076645929}
{1: 0.04131910919281311, 2: 0.0016351496179347956, 3: 0.0030716239012411896}
{1: 0.1771613350916689, 2: 0.13534049221543457, 3: 0.013723919694781556}
k=2 æ—¶çš„æ¦‚ç‡å¯†åº¦
{1: 0.03178787537795644, 2: 0.16427006941590264, 3: 0.14466794867681504}
{1: 0.026792930905745034, 2: 0.003220749147742275, 3: 0.005967511370200231}
{1: 0.22110756108533755, 2: 0.26802033748210696, 3: 0.026840350461291625}
k=3 æ—¶çš„æ¦‚ç‡å¯†åº¦
{1: 0.00879596401261926, 2: 0.23162580018270137, 3: 0.15009942277353658}
{1: 0.017964386176866163, 2: 0.0040412635615593925, 3: 0.008591866296713964}
{1: 0.01102829558991302, 2: 0.3638189115792198, 3: 0.0354162032665907}
k=4 æ—¶çš„æ¦‚ç‡å¯†åº¦
{1: 0.010231864737074553, 2: 0.23905049106054166, 3: 0.1874814465698455}
{1: 0.013391533141938477, 2: 0.005348468289115052, 3: 0.011381430132195194}
{1: 0.00919145633055199, 2: 0.3362516370263627, 3: 0.040106087762924514}
k=5 æ—¶çš„æ¦‚ç‡å¯†åº¦
{1: 0.010953452694776046, 2: 0.1393888918855546, 3: 0.11134270190658971}
{1: 0.004295766694553839, 2: 0.006489497138077377, 3: 0.010832627307478103}
{1: 0.007823205569486222, 2: 0.12488416706442222, 3: 0.039732878044842136}
```

### 4 KNNå®æˆ˜

#### 4-1 å®éªŒç›®çš„

æŒæ¡KNNç®—æ³•çš„ä½¿ç”¨ã€‚

**ä¸€ã€æ•°æ®é¢„å¤„ç†**

1.å°†e2.txtä¸­çš„æ•°æ®å¤„ç†æˆå¯ä»¥è¾“å…¥ç»™æ¨¡å‹çš„æ ¼å¼

2.æ˜¯å¦è¿˜éœ€è¦å¯¹ç‰¹å¾å€¼è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Ÿç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ

**äºŒã€æ•°æ®å¯è§†åŒ–åˆ†æ**

å°†é¢„å¤„ç†å¥½çš„æ•°æ®ä»¥æ•£ç‚¹å›¾çš„å½¢å¼è¿›è¡Œå¯è§†åŒ–ï¼Œé€šè¿‡ç›´è§‚æ„Ÿè§‰æ€»ç»“è§„å¾‹ï¼Œæ„Ÿå— KNN æ¨¡å‹æ€æƒ³ä¸äººç±»ç»éªŒçš„ç›¸ä¼¼ä¹‹å¤„ã€‚

**ä¸‰ã€æ„å»º KNN æ¨¡å‹å¹¶æµ‹è¯•**

1.è¾“å‡ºæµ‹è¯•é›†å„æ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾ï¼Œå¹¶è®¡ç®—æ¨¡å‹å‡†ç¡®ç‡ã€‚

2.é€‰æ‹©å“ªç§è·ç¦»æ›´å¥½ï¼Ÿæ¬§æ°è¿˜æ˜¯é©¬æ°ï¼Ÿ

3.æ”¹å˜æ•°æ®é›†çš„åˆ’åˆ†ä»¥åŠ k çš„å€¼ï¼Œè§‚å¯Ÿæ¨¡å‹å‡†ç¡®ç‡éšä¹‹çš„å˜åŒ–æƒ…å†µã€‚

æ³¨æ„ï¼šé€‰æ‹©è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„éšæœºæ€§

**å››ã€ä½¿ç”¨æ¨¡å‹æ„å»ºå¯ç”¨ç³»ç»Ÿ**

åˆ©ç”¨æ„å»ºå¥½çš„ KNN æ¨¡å‹å®ç°ç³»ç»Ÿï¼Œè¾“å…¥ä¸ºæ–°çš„æ•°æ®çš„ä¸‰ä¸ªç‰¹å¾ï¼Œè¾“å‡ºä¸ºé¢„æµ‹çš„ç±»åˆ«ã€‚

#### 4-2 å®éªŒè¿‡ç¨‹

é¦–å…ˆè·å¾—è¯¥æ•°æ®ï¼Œåˆ©ç”¨`pandas`çš„`read_csv`å‡½æ•°è¿›è¡Œè¯»å…¥ï¼Œé€šè¿‡å¯¹æ•°æ®çš„è§‚å¯Ÿï¼Œå¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªæš´åŠ›çš„å½’ä¸€åŒ–å¤„ç†ä»¥åŠæ ‡ç­¾çš„æ•°æ®åŒ–ã€‚å¦‚ä¸‹ï¼š

```python
yMapping = {'didntLike': 0, 'smallDoses': 1, 'largeDoses': 2}
yList = ['didntLike', 'smallDoses', 'largeDoses']


def dealForData(data):
    data['x0'] = data['x0'] / 10000
    data['x1'] = data['x1'] / 10
    data['y'] = data['y'].replace(yMapping)
    return data

data = pd.read_csv('../../data/e2.txt', sep="\t", names=['x0', 'x1', 'x2', 'y'])
data = dealForData(data)
```

éšåç®€å•å¯¹æ•°æ®é›†è¿›è¡Œä¸€ä¸ª4ï¼š1çš„åˆ’åˆ†ï¼Œå¹¶å¯¹è®­ç»ƒé›†ç»˜åˆ¶æ•£ç‚¹å›¾æ¥å¯è§†åŒ–

```python
def DrawScatterPlot(X, y):
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    # ä¸ºæ¯ä¸ªåˆ†ç±»çš„ç‚¹è®¾ç½®ä¸åŒçš„é¢œè‰²å’Œæ ‡ç­¾
    colors = ['red', 'green', 'blue']
    labels = ['didntLike', 'smallDoses', 'largeDoses']
    for i in range(3):
        subset = X[y == i]
        ax.scatter(subset['x0'], subset['x1'], subset['x2'], c=colors[i], label=labels[i])
    # è®¾ç½®å›¾ä¾‹å’Œåæ ‡è½´æ ‡ç­¾
    ax.legend()
    ax.set_xlabel('X0')
    ax.set_ylabel('X1')
    ax.set_zlabel('X2')
    plt.show()

def splitForData(data, test_size, random_state):
    np.random.seed(random_state)
    data_shuffled = data.sample(frac=1).reset_index(drop=True)
    train_size = int((1 - test_size) * len(data))
    train_data = data_shuffled[:train_size]
    test_data = data_shuffled[train_size:]
    X_train = train_data.drop('y', axis=1)
    y_train = train_data['y']
    X_test = test_data.drop('y', axis=1)
    y_test = test_data['y'].copy()
    return X_train, y_train, X_test, y_test

X_train, y_train, X_test, y_test = splitForData(data, 0.2, 7)
y_test = y_test.to_numpy()
DrawScatterPlot(X_train,y_train)
```

å³å¯è·å¾—å¦‚å³å›¾çš„æ•£ç‚¹å›¾ï¼š

![](G:\ExpMachineLearn\ExpML\Exp2\images\part4æ•£ç‚¹å›¾1.png)

éšåå³å¯æ‰§è¡ŒKNNç®—æ³•ï¼Œåœ¨ä¸Šè¿°ç»™å®šçš„ä»£ç ä¸­ï¼Œæˆ‘é‡‡ç”¨å¯è°ƒèŠ‚çš„å‚æ•°æ¥è¿›è¡Œé€‰æ‹©ä¸åŒçš„è·ç¦»è®¡ç®—æ–¹å¼ä»¥åŠä¸åŒçš„è®¡ç®—æ¦‚ç‡å¯†åº¦çš„æ–¹å¼ï¼Œå¹¶åœ¨é¢„æµ‹ä¸­å†ä¹˜ç»Ÿè®¡çš„å…ˆéªŒæ¦‚ç‡æ¥è¿›è¡Œé¢„æµ‹ï¼Œæ¯”å•çº¯é‡‡ç”¨ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦çš„é¢„æµ‹åœ¨æ•°æ®ä¸ç¨³å®šæ‰“ä¹±çš„æƒ…å†µä¸‹æ›´å¥½ã€‚KNNç±»çš„å…·ä½“æè¿°å¦‚ä¸‹ã€‚

```python
class kNNClassifier:
    
    # è¯¥æ–¹æ³•åŸºäºä¸Šè¿°ç¬¬ä¸‰é—®é¢˜çš„KNNä¸­çš„ç¬¬ä¸€ç§ç†è§£ï¼Œå³å°†ç±»åˆ«åˆ†ä¸ºä¸åŒçš„ç±»è¿›è¡Œï¼Œä¸åŒç±»æ¯”ä¹‹é—´äº’ä¸å¹²æ¶‰ï¼Œä½†æ˜¯è¯¥æ–¹æ³•å­˜åœ¨æŸä¸€ç±»çš„ç‚¹å°‘äºkä¸ªï¼Œåˆ™ä¼šå‡ºç°æ¦‚å¿µé—®é¢˜ï¼Œå¹¶ä¸”ä¸æ˜“è§£å†³ï¼Œå› æ­¤é‡‡ç”¨è¯¥æ–¹æ³•æ—¶kä¸åº”è¯¥é€‰å–å¤ªå¤§ã€‚
    def KNN_Vn(self, X, k, t):
        '''
        åŸºäº p_{n}(x) = k_{n}/(V_{n}*n) æ¥è®¡ç®—æ¦‚ç‡å¯†åº¦
        :param X: åŸå§‹æ•°æ®é›†çš„æŸä¸€ç±»åˆ«
        :param k: KNN çš„k
        :param t: æµ‹è¯•é›†
        :return: ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°
        '''
        res = np.zeros((t.shape[0],))
        for i, test in enumerate(t):
            dist = np.linalg.norm(X - test, axis=1)
            dist = np.sort(dist)
            if dist[k - 1] != 0:
                res[i] = k / (X.shape[0] * dist[k - 1] ** X.shape[1])
            else:
                res[i] = k / (X.shape[0] * (1e-5) ** X.shape[1])
        return res

   # è¯¥æ–¹æ³•ä¸ä¸Šè¿°çš„æ–¹æ³•ä¸ºé…å¥—ä½¿ç”¨çš„æ–¹æ³•ï¼Œæ¥å®ç°ä¸Šè¿°æè¿°çš„ç†è§£ã€‚
    def kNN_Vn(self, X_train, y_train, X_test, k):
        res = [{} for _ in range(X_test.shape[0])]
        yValueList = y_train.unique().tolist()
        for y in yValueList:
            trainSet = X_train[y_train == y].copy()
            yRes = self.KNN_Vn(trainSet, k, X_test)
            for i in range(len(yRes)):
                # ä¸º res ä¸­çš„æ¯ä¸€è¡Œçš„å­—å…¸æ·»åŠ é”®å€¼å¯¹ï¼Œé”®ä¸º yï¼Œå€¼ä¸º yRes ä¸­å¯¹åº”çš„å…ƒç´ 
                res[i][y] = yRes[i]
        return res

   # è®¡ç®—è¶…çƒä½“ç³»æ•°ä¸­çš„gammaå‡½æ•°
    def gamma(self, x):
        # é€’å½’åˆ©ç”¨å·²çŸ¥çš„Gamma(1/2) = sqrt(pi)
        if abs(x - 0.5) < 1e-6:
            return math.sqrt(math.pi)
        elif abs(x - 1) < 1e-6:
            return 1
        else:
            return (x - 1) * self.gamma(x - 1)

   # å¸¸è§„åŸºäºæ¬§æ°è·ç¦»æ•°ç‚¹çš„è®¡ç®—æ–¹å¼
    def kNN_Euler_Count(self, X_train, y_train, X_test, k):
        '''
        åŸºäºæ¬§æ‹‰è·ç¦»çš„è®¡æ•°æ–¹æ³•æ¥è®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        :param X_train: è®­ç»ƒé›†
        :param y_train: è®­ç»ƒæ ‡ç­¾
        :param X_test: æµ‹è¯•é›†
        :param k: kè¿‘é‚»çš„k
        :return: æµ‹è¯•é›†çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        '''
        res = []
        # è¶…çƒä½“ä½“ç§¯ç³»æ•° $V_n(r) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} r^n$ ä»¤ $ alpha = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} $
        pi_power = math.pi ** (X_train.shape[1] / 2)
        gamma_value = self.gamma(((X_train.shape[1] / 2) + 1))
        alpha = ((pi_power) / gamma_value) if gamma_value != 0 else float('inf')
        for i, test in enumerate(X_test):
            dist = np.linalg.norm(X_train - test, axis=1)
            KNN_indices = np.argsort(dist)[:k]
            KNN_labels = y_train[KNN_indices]
            d = dist[KNN_indices[-1]]
            if d == 0:
                d = 1e-5
            class_probs = {
                cls: np.sum(KNN_labels == cls) / (X_train.shape[0] * alpha * (d ** X_train.shape[1]))
                for cls in np.unique(y_train)}
            res.append(class_probs)
        return res

   # é©¬æ°è·ç¦»çš„è®¡ç®—
    def mahalanobis_distance(self, x, dataset, invCov):
        '''
        è®¡ç®—å•ä¸ªæ ·æœ¬ä¸æ•°æ®é›†æ‰€æœ‰æ ·æœ¬çš„é©¬æ°è·ç¦»
        :param x:
        :param dataset:
        :param invCov:
        :return:
        '''
        dist = np.zeros(dataset.shape[0])
        dataset = dataset.to_numpy()
        for i, data in enumerate(dataset):
            delta = x - data
            dist[i] = np.sqrt(delta.dot(invCov).dot(delta.T))
        return dist

   # åŸºäºé©¬æ°è·ç¦»çš„KNN
    def kNN_Mahalanobis_Count(self, X_train, y_train, X_test, k):
        '''
        åŸºäºé©¬æ°è·ç¦»çš„è®¡æ•°æ–¹æ³•æ¥è®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        :param X_train: è®­ç»ƒé›†
        :param y_train: è®­ç»ƒæ ‡ç­¾
        :param X_test: æµ‹è¯•é›†
        :param k: kè¿‘é‚»çš„k
        :return: æµ‹è¯•é›†çš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦
        '''
        # è¶…çƒä½“ä½“ç§¯ç³»æ•° $V_n(r) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} r^n$ ä»¤ $ alpha = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} $
        pi_power = math.pi ** (X_train.shape[1] / 2)
        gamma_value = self.gamma((X_train.shape[1] / 2) + 1)
        alpha = ((pi_power) / gamma_value) if gamma_value != 0 else float('inf')
        cov = np.cov(X_train.T)
        invCov = inv(cov)
        res = []
        for i, test in enumerate(X_test):
            dist = self.mahalanobis_distance(test, X_train, invCov)
            KNN_indices = np.argsort(dist)[:k]
            KNN_labels = y_train[KNN_indices]
            d = dist[KNN_indices[-1]]
            if d == 0:
                d = 1e-5
            class_probs = {
                cls: np.sum(KNN_labels == cls) / (X_train.shape[0] * alpha * (d ** X_train.shape[1]))
                for cls in np.unique(y_train)}
            res.append(class_probs)
        return res

   # predictå‡½æ•°
    def execute(self, X_train, y_train, X_test, k, typ=0):
        yValueList = y_train.unique().tolist()
        prior_prob = {}
        for y in yValueList:
            prior_prob[y] = X_train[y_train == y].shape[0]
        if typ == 0:
            probs = self.kNN_Euler_Count(X_train, y_train, X_test.to_numpy(), k)
        elif typ == 1:
            probs = self.kNN_Mahalanobis_Count(X_train, y_train, X_test.to_numpy(), k)
        elif typ == 2:
            probs = self.kNN_Vn(X_train, y_train, X_test.to_numpy(), k)

        predict = np.zeros(X_test.shape[0])
        # print(probs)
        for i, class_prob in enumerate(probs):
            max_prob = -1
            for y in class_prob:
                current_prob = class_prob[y] * prior_prob[y]
                if current_prob > max_prob:
                    max_prob = current_prob
                    predict[i] = y
        return predict

   # å•çº¯è®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•°
    def density(self, X_train, y_train, X_test, k, typ=0):
        yValueList = y_train.unique().tolist()
        prior_prob = {}
        if not isinstance(X_test, np.ndarray):
            # å¦‚æœä¸æ˜¯ï¼Œè½¬æ¢å®ƒä¸ºnumpyæ•°ç»„
            X_test = np.array(X_test)
        for y in yValueList:
            prior_prob[y] = X_train[y_train == y].shape[0]
        if typ == 0:
            probs = self.kNN_Euler_Count(X_train, y_train, X_test, k)
        elif typ == 1:
            probs = self.kNN_Mahalanobis_Count(X_train, y_train, X_test, k)
        elif typ == 2:
            probs = self.kNN_Vn(X_train, y_train, X_test, k)
        return probs
```

éšååœ¨éšæœºç§å­ï¼Œkä¿æŒä¸å˜çš„æƒ…å†µä¸‹ï¼Œåœ¨`execute` æ–¹æ³•ä¸­æ‰§è¡Œä¸åŒçš„`typ` å³å¯è¿”å›ä¸åŒçš„é¢„æµ‹ç»“æœã€‚

```python
# åŸºäºä¸€èˆ¬æ¬§æ°è·ç¦»æ–¹æ³•
y_pred = knn.execute(X_train, y_train, X_test,5,0)
accuracy = np.mean(y_pred == y_test)
print(f'ä¸€èˆ¬æ¬§å¼-Accuracy: {accuracy}')

# åŸºäºé©¬æ°è·ç¦»æ–¹æ³•
y_pred = knn.execute(X_train, y_train, X_test,5,1)
accuracy = np.mean(y_pred == y_test)
print(f'ä¸€èˆ¬é©¬æ°-Accuracy: {accuracy}')

# åŸºäºå•ç±»æ¬§å¼è·ç¦»çš„æ–¹æ³•
y_pred = knn.execute(X_train, y_train, X_test,5,2)
accuracy = np.mean(y_pred == y_test)
print(f'ç‹¬ç±»æ¬§å¼-Accuracy: {accuracy}')
```

åœ¨æ•°æ®é›†åˆåˆ’åˆ†ä¸ºéšæœºç§å­ä¸º7æ—¶ï¼Œç»“æœå¦‚ä¸‹ï¼š

```txt
ä¸€èˆ¬æ¬§å¼-Accuracy: 0.995
ä¸€èˆ¬é©¬æ°-Accuracy: 0.965
ç‹¬ç±»æ¬§å¼-Accuracy: 0.98
```

å¯ä»¥å‘ç°æ­¤æ—¶é‡‡ç”¨æ¬§å¼è·ç¦»çš„å‡†ç¡®ç‡å¤§äºé©¬æ°è·ç¦»ã€‚

å†é‡‡ç”¨ä¸åŒçš„éšæœºç§å­ï¼Œä¾‹å¦‚21ï¼Œ42ï¼Œ56

å½“randstate=21æ—¶

```
ä¸€èˆ¬æ¬§å¼-Accuracy: 0.98
ä¸€èˆ¬é©¬æ°-Accuracy: 0.95
ç‹¬ç±»æ¬§å¼-Accuracy: 0.985
```

å½“randstate=42æ—¶

```
ä¸€èˆ¬æ¬§å¼-Accuracy: 0.97
ä¸€èˆ¬é©¬æ°-Accuracy: 0.96
ç‹¬ç±»æ¬§å¼-Accuracy: 0.975
```

å½“randstate=56æ—¶

```
ä¸€èˆ¬æ¬§å¼-Accuracy: 0.945
ä¸€èˆ¬é©¬æ°-Accuracy: 0.94
ç‹¬ç±»æ¬§å¼-Accuracy: 0.945
```

æ€»çš„æ¥çœ‹ï¼Œæ¬§å¼è·ç¦»åº”å½“ä¼˜äºé©¬æ°è·ç¦»åœ¨è¯¥é—®é¢˜ä¸Šã€‚

éšåæ‰§è¡Œè¿™ä¸¤ä¸ªå‡½æ•°æ¥ç»˜åˆ¶å‡†ç¡®ç‡æ ¹æ®ä¸åŒçš„åˆ’åˆ†ï¼ˆæµ‹è¯•é›†å æ¯”ï¼‰å’Œä¸åŒkçš„å˜åŒ–æˆéƒ½

```python
def plotAccuracyK(X_train, y_train, X_test, y_test):
    x = np.linspace(1,20,20)
    accuracy = np.zeros(20)
    for k in range(20):
        knn = kNNClassifier()
        y_pred = knn.execute(X_train, y_train, X_test,5, 0)
        accuracy[k]=np.mean(y_pred == y_test)
    plt.plot(x, accuracy,'r-')
    plt.xlabel('k')
    plt.ylabel('accuracy')
    plt.show()

def plotAccuracySize(data,random_state=42):
    x = np.linspace(0.05,0.4,8)
    accuracy = np.zeros(8)
    for i,size in enumerate(x):
        X_train, y_train, X_test, y_test = splitForData(data, size, 7)
        y_test = y_test.to_numpy()
        knn = kNNClassifier()
        y_pred = knn.execute(X_train, y_train, X_test,5, 0)
        accuracy[i] = np.mean(y_pred == y_test)
    plt.plot(x, accuracy, 'r-')
    plt.xlabel('size')
    plt.ylabel('accuracy')
    plt.show()

plotAccuracyK(X_train, y_train, X_test, y_test)
plotAccuracySize(data)
```

å¾—åˆ°å¦‚ä¸‹ç»“æœï¼š

![](G:\ExpMachineLearn\ExpML\Exp2\images\part4acc&k.png)

![](G:\ExpMachineLearn\ExpML\Exp2\images\part4acc&size.png)

å¯ä»¥å‘ç°éšç€kçš„å¢é•¿ï¼ŒKNNçš„å‡†ç¡®ç‡å˜åŒ–ä¸å¤§åœ¨è¯¥æ•°æ®é›†ä¸‹ã€‚

ä½†éšç€æµ‹è¯•é›†å æ¯”è¶Šæ¥è¶Šå¤§ï¼ŒKNNçš„å‡†ç¡®ç‡é€æ­¥ä¸‹é™ã€‚

éšåè¾“å…¥è¿™æ ·çš„æ•°æ®ï¼ŒæŸ¥çœ‹å…¶æ˜¯å¦å¯ä»¥é¢„æµ‹

```python
Temp = [40000,8,0.9]
Temp = pd.DataFrame([Temp], columns=['x0', 'x1', 'x2'])
y_pred = knn.execute(X_train, y_train, Temp,5,1)
index = int(y_pred[0])  # è½¬æ¢ä¸ºæ•´æ•°
print(yList[index]) 
```

å…¶è¾“å‡º `didntLike` å³å¯ä»¥æ­£å¸¸è¿›è¡Œé¢„æµ‹ã€‚

å®éªŒåˆ°æ­¤ç»“æŸã€‚

## ä»£ç ç»“æ„

```
/Exp2/
|------- code/
|		|------- Part1 ç¬¬ä¸€éƒ¨åˆ†çš„ä»£ç 
|		|		|------- DataProcessor.py æ•°æ®å¤„ç†çš„å·¥å…·ç±»
|		|		|------- exp2-1.py çº¿æ€§åˆ†ç±»ä¸»å‡½æ•°ä»£ç 
|		|		|------- LinearClassifier.py çº¿æ€§åˆ†ç±»ç±»
|		|------- Part2 ç¬¬äºŒéƒ¨åˆ†çš„ä»£ç 
|		|		|------- exp2-2.py æœ€å¤§ä¼¼ç„¶ä¸»å‡½æ•°ä»£ç 
|		|------- Part3 ç¬¬ä¸‰éƒ¨åˆ†çš„ä»£ç 
|		|		|------- exp2-3-kNearestNeighbor.py KNNæ‰§è¡Œä¸»ä»£ç 
|		|		|------- exp2-3-ParzenWindow.py Parzençª—æ‰§è¡Œä¸»ä»£ç 
|		|------- Part4 ç¬¬å››éƒ¨åˆ†ä»£ç 
|		|		|------- exp2-4.py KNNå®æˆ˜æ‰§è¡Œä¸»ä»£ç 
|		|		|------- KNN.py part3å’Œpart4æœ‰å…³KNNçš„KNNç±»
|
|-------- data/
|		|------- e2data1.mat part1æ•°æ®
|		|------- exp2-2.xlsx part2æ•°æ®
|		|------- exp2-3.xlsx part3æ•°æ®
|		|------- e2.txt part4æ•°æ®
|
|-------- images/å®éªŒæŠ¥å‘Šæœ‰å…³å›¾ç‰‡
|
|-------- Exp2.md å®éªŒæŠ¥å‘ŠMarkdown
|
|-------- Exp2.pdf å®éªŒæŠ¥å‘Špdf
```

## å¿ƒå¾—ä½“ä¼š

åœ¨è¿™æ¬¡æœºå™¨å­¦ä¹ çš„åŸºç¡€å®éªŒä¸­ï¼Œæˆ‘æœ‰å¹¸æ·±å…¥æ¢è®¨äº†å››ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼šç®€å•çº¿æ€§åˆ†ç±»æ¨¡å‹ã€æœ‰å‚ä¼°è®¡ã€å¤šç»´ç»Ÿè®¡åˆ†æåŠæ— å‚ä¼°è®¡ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è®©æˆ‘è·å¾—äº†å®è´µçš„å­¦æœ¯åŠå®è·µç»éªŒã€‚

**ç®€å•çº¿æ€§åˆ†ç±»æ¨¡å‹**ï¼š é€šè¿‡å®ç°ç®€å•çš„çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼Œæˆ‘ä¸ä»…å¤ä¹ äº†çº¿æ€§ä»£æ•°çš„åŸºæœ¬çŸ¥è¯†ï¼Œè¿˜å­¦ä¹ äº†å¦‚ä½•é€šè¿‡ç¼–ç¨‹å°†ç†è®ºåº”ç”¨åˆ°å®é™…æ•°æ®åˆ†æä¸­ã€‚è¿™ä¸€éƒ¨åˆ†çš„æŒ‘æˆ˜åœ¨äºé€‰æ‹©åˆé€‚çš„æ¨¡å‹å‚æ•°å’Œç†è§£æ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚å®éªŒè¿‡ç¨‹ä¸­ï¼Œæˆ‘é€šè¿‡å¯è§†åŒ–æ‰‹æ®µç›´è§‚åœ°è§‚å¯Ÿäº†åˆ†ç±»æ•ˆæœï¼Œè¿™æå¤§åœ°å¢å¼ºäº†æˆ‘çš„ç›´è§‚ç†è§£å’Œå¯¹æ¨¡å‹è°ƒä¼˜çš„å®è·µèƒ½åŠ›ã€‚

**æœ‰å‚ä¼°è®¡ä¸­çš„æå¤§ä¼¼ç„¶ä¼°è®¡**ï¼š åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘å­¦ä¹ äº†å¦‚ä½•åœ¨å•ç»´å’Œå¤šç»´æƒ…å¢ƒä¸‹ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡æ¥è®¡ç®—æ•°æ®çš„å‡å€¼å’Œæ–¹å·®ã€‚é€šè¿‡å¯¹æ¯”å•ç»´å’Œå¤šç»´çš„è®¡ç®—æ–¹æ³•ï¼Œæˆ‘æ›´æ·±å…¥åœ°ç†è§£äº†è¿™äº›ç»Ÿè®¡é‡åœ¨ä¸åŒç»´åº¦ä¸‹çš„è¡Œä¸ºå’Œæ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å®é™…æ•°æ®é›†æ—¶å¦‚ä½•åº”ç”¨è¿™äº›ç†è®ºæ¥æå–æœ‰ç”¨çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

**æ— å‚ä¼°è®¡çš„Parzençª—å’ŒKNNç®—æ³•**ï¼š æ¢ç´¢æ— å‚ä¼°è®¡çš„æ–¹æ³•å¼€é˜”äº†æˆ‘çš„è§†é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Parzençª—å’ŒKNNç®—æ³•è¿›è¡Œæ•°æ®åˆ†ç±»å’Œå›å½’åˆ†ææ–¹é¢ã€‚æˆ‘å®è·µäº†å¦‚ä½•æ ¹æ®æ•°æ®çš„åˆ†å¸ƒé€‰æ‹©åˆé€‚çš„çª—å£å¤§å°å’Œé‚»å±…æ•°ï¼Œè¿™å¯¹äºä¼˜åŒ–æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚é€šè¿‡å®é™…æ“ä½œï¼Œæˆ‘å­¦ä¼šäº†è°ƒæ•´è¿™äº›å‚æ•°ä»¥é€‚åº”ä¸åŒçš„æ•°æ®é›†ï¼Œè¿›è€Œä¼˜åŒ–åˆ†ç±»å’Œé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚

**æ•´ä½“åæ€**ï¼š è¿™å››ä¸ªå®éªŒéƒ¨åˆ†ä½¿æˆ‘è®¤è¯†åˆ°ï¼Œæ— è®ºæ˜¯æœ‰å‚è¿˜æ˜¯æ— å‚ä¼°è®¡ï¼Œç†è§£å…¶èƒŒåçš„æ•°å­¦åŸç†å’Œå¦‚ä½•å°†è¿™äº›åŸç†åº”ç”¨äºå®é™…é—®é¢˜éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚æ­¤å¤–ï¼Œå®éªŒä¸ä»…æé«˜äº†æˆ‘çš„ç¼–ç¨‹æŠ€èƒ½ï¼Œè¿˜å¢å¼ºäº†æˆ‘å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹å¦‚ä½•åœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨çš„è®¤è¯†ã€‚æˆ‘å¯¹æœªæ¥åœ¨æ›´å¤æ‚æ•°æ®é›†ä¸Šåº”ç”¨è¿™äº›æŠ€æœ¯æ„Ÿåˆ°å…´å¥‹ï¼Œå¹¶æœŸå¾…åœ¨æœªæ¥çš„å­¦ä¹ å’Œç ”ç©¶ä¸­ç»§ç»­æ¢ç´¢æ›´å¤šæœºå™¨å­¦ä¹ çš„é¢†åŸŸã€‚
