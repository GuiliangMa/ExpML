# æœºå™¨å­¦ä¹ åŸºç¡€ å®éªŒä¸€ å®éªŒæŠ¥å‘Š

2021çº§è½¯ä»¶5ç­ é©¬è´µäº® 202122202214

[TOC]

## å®éªŒç›®çš„

æœ¬å®éªŒä»¥è´·æ¬¾è¿çº¦ä¸ºèƒŒæ™¯ï¼Œè¦æ±‚ä½¿ç”¨è´å¶æ–¯å†³ç­–è®ºçš„ç›¸å…³çŸ¥è¯†åœ¨è®­ç»ƒé›†ä¸Šæ„å»ºæ¨¡å‹ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè´·æ¬¾è¿çº¦é¢„æµ‹å¹¶è®¡ç®—åˆ†ç±»å‡†ç¡®åº¦ã€‚

## å®éªŒè¦æ±‚

1. å®éªŒä¸é™åˆ¶ä½¿ç”¨ä½•ç§é«˜çº§è¯­è¨€ï¼Œæ¨èä½¿ç”¨pythonä¸­pandasåº“å¤„ç†csvæ–‡ä»¶ã€‚
   - å®‰è£…ï¼špip install pandas/conda install pandasã€åœ¨ä½¿ç”¨condaå‘½ä»¤ï¼Œéœ€å®‰è£…anacondaç¯å¢ƒã€‘
   - å¯¼å…¥ï¼šimport pandas as pdã€å»ºè®®ã€‘
2. åœ¨è¿›è¡Œè´å¶æ–¯åˆ†ç±»ä¹‹å‰é‡ç‚¹æ˜¯å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†æ“ä½œï¼Œå¦‚ï¼Œç¼ºå¤±å€¼çš„å¡«å……ã€å°†æ–‡å­—è¡¨è¿°è½¬ä¸ºæ•°å€¼å‹ã€æ—¥æœŸå¤„ç†æ ¼å¼ï¼ˆå¤„ç†æˆâ€œå¹´-æœˆ-æ—¥â€ä¸‰åˆ—å±æ€§æˆ–è€…ä»¥æœ€æ—©æ—¶é—´ä¸ºåŸºå‡†è®¡ç®—å·®å€¼ï¼‰ã€æ— å…³å±æ€§çš„åˆ é™¤ã€å¤šåˆ—æ•°æ®èåˆç­‰æ–¹é¢ã€‚
3. æ•°æ®ä¸­å­˜åœ¨å¤§é‡è¿ç»­å€¼çš„å±æ€§ï¼Œä¸èƒ½ç›´æ¥è®¡ç®—ä¼¼ç„¶ï¼Œéœ€è¦å°†è¿ç»­å±æ€§ç¦»æ•£åŒ–ã€‚
4. å¦å¤–ï¼Œç‰¹åˆ«æ³¨æ„é›¶æ¦‚ç‡é—®é¢˜ï¼Œè´å¶æ–¯ç®—æ³•ä¸­å¦‚æœä¹˜ä»¥0çš„è¯å°±ä¼šå¤±å»æ„ä¹‰ï¼Œéœ€è¦ä½¿ç”¨å¹³æ»‘æŠ€æœ¯ã€‚ã€å¯ä»¥ç™¾åº¦äº†è§£ä¸€ä¸‹æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ã€‘
5. å®éªŒç›®çš„æ˜¯ä½¿ç”¨è´å¶æ–¯å¤„ç†å®é™…é—®é¢˜ï¼Œä¸å¾—ä½¿ç”¨ç°æˆå·¥å…·åŒ…ç›´æ¥è¿›è¡Œåˆ†ç±»ã€‚ã€è¯¥ç‚¹åˆ‡è®°ï¼ï¼ï¼è¿™ä¸ªä¸€å®šè¦è‡ªå·±å†™ï¼Œæ‰èƒ½æ„Ÿå—è´å¶æ–¯çš„é­…åŠ›ã€‘
6. å®éªŒä»£ç ä¸­éœ€è¦æœ‰å¿…è¦çš„æ³¨é‡Šï¼Œå…·æœ‰è‰¯å¥½çš„å¯è¯»æ€§ã€‚

## å®éªŒè¿‡ç¨‹

### 1.æ•°æ®ç®€æ˜“åˆ†æ

åœ¨è¿›è¡Œè´å¶æ–¯å†³ç­–ä¹‹å‰ï¼Œæˆ‘åº”å½“å…ˆå¯¹è®­ç»ƒæ•°æ®è¿›è¡Œä¸€ä¸ªæ•´ä½“çš„åˆ†æã€‚å¯¹æ­¤æˆ‘ç¼–å†™äº†å¦‚ä¸‹ä»£ç æ¥ç²—ç•¥æ£€æµ‹è®­ç»ƒæ•°æ®ï¼ˆtrain.csvï¼‰æ¯ä¸ªç‰¹å¾çš„åŸºæœ¬æƒ…å†µï¼Œå³ç¼ºå¤±æ€§ï¼Œæ•°å€¼ç±»å‹ï¼Œç‰¹å¾å€¼ä¸ªæ•°ç­‰ä¿¡æ¯ã€‚å¾—åˆ°å¦‚ä¸‹çš„è¡¨æ ¼ã€‚

```python
'''
DataAnaslyze.py (Part1)
'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataAnasly = {}

def checkColumn(columns):
    for column in columns:
        dataAnasly[column]={}
        data = df[column]
        values_counts = data.value_counts()
        dataAnasly[column]['count'] = len(values_counts)

        if(len(values_counts) < 30):
            unique_values = df[column].drop_duplicates().tolist()
            dataAnasly[column]['unique'] = unique_values
        else:
            dataAnasly[column]['unique'] = []

        dtype = df[column].dtype
        dataAnasly[column]['dtype'] = dtype

        has_null = df[column].isnull().any()
        dataAnasly[column]['has_null'] = has_null

    analysis_df = pd.DataFrame(dataAnasly).T
    analysis_df.to_excel('../data/æ•°æ®åˆ†æå™¨.xlsx')

train_data_path = '../data/train.csv'
df = pd.read_csv(train_data_path)
checkColumn(df.columns.tolist())

```

æ‰§è¡Œè¿™éƒ¨åˆ†ä»£ç ï¼Œå¯ä»¥å¾—åˆ° æ•°æ®åˆ†æå™¨.xlsx æ–‡ä»¶ï¼Œç‚¹å‡»æŸ¥çœ‹åå¯ä»¥è·å¾—å¦‚ä¸‹è¡¨æ ¼æ¥å¯¹ç‰¹å¾è¿›è¡Œåˆ†æã€‚åœ¨`unique`ä¸€æ ä¸­ï¼Œä»…å±•ç¤ºäº†å–å€¼ç§ç±»å°äºç­‰äº30ä¸ªçš„ç§ç±»çš„å–å€¼ã€‚éšåçš„`type` ä¸€æ ä¸­ï¼Œä¸æ•°å€¼æœ‰å…³çš„ç±»å‹å‡ä¸º `object`ï¼Œå…¶ä½™å·²ç»è¢«æˆåŠŸåˆ†æˆ`int64`å’Œ`float64`ä¸¤ç±»ã€‚

| columnName               | count | unique                                                       | type       | has_null |
| ------------------------ | ----- | ------------------------------------------------------------ | ---------- | -------- |
| loan_id                  | 9000  | []                                                           | int64      | FALSE    |
| user_id                  | 9000  | []                                                           | int64      | FALSE    |
| total_loan               | 1526  | []                                                           | float64    | FALSE    |
| year_of_loan             | 2     | [3, 5]                                                       | int64      | FALSE    |
| interest                 | 1001  | []                                                           | float64    | FALSE    |
| monthly_payment          | 5872  | []                                                           | float64    | FALSE    |
| class                    | 7     | ['B', 'C', 'A', 'G', 'D', 'E', 'F']                          | **object** | FALSE    |
| employer_type            | 6     | ['å¹¼æ•™ä¸ä¸­å°å­¦æ ¡', 'æ”¿åºœæœºæ„', 'ä¸Šå¸‚ä¼ä¸š', 'æ™®é€šä¼ä¸š', 'ä¸–ç•Œäº”ç™¾å¼º', 'é«˜ç­‰æ•™è‚²æœºæ„'] | **object** | FALSE    |
| industry                 | 14    | ['é‡‘èä¸š', 'å›½é™…ç»„ç»‡', 'æ–‡åŒ–å’Œä½“è‚²ä¸š', 'å»ºç­‘ä¸š', 'ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§ä¾›åº”ä¸š', 'æ‰¹å‘å’Œé›¶å”®ä¸š', 'é‡‡çŸ¿ä¸š', 'æˆ¿åœ°äº§ä¸š',  'äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š', 'å…¬å…±æœåŠ¡ã€ç¤¾ä¼šç»„ç»‡', 'ä½å®¿å’Œé¤é¥®ä¸š', 'ä¿¡æ¯ä¼ è¾“ã€è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š', 'å†œã€æ—ã€ç‰§ã€æ¸”ä¸š', 'åˆ¶é€ ä¸š'] | **object** | FALSE    |
| work_year                | 11    | [nan, '8 years', '10+ years', '3 years', '7 years', '4 years', '1 year',  '< 1 year', '6 years', '2 years', '9 years', '5 years'] | **object** | **TRUE** |
| house_exist              | 5     | [2, 1, 0, 3, 4]                                              | int64      | FALSE    |
| censor_status            | 3     | [2, 0, 1]                                                    | int64      | FALSE    |
| issue_date               | 126   | []                                                           | **object** | FALSE    |
| use                      | 14    | [0, 4, 2, 9, 5, 3, 6, 1, 7, 8, 10, 12, 11, 13]               | int64      | FALSE    |
| post_code                | 786   | []                                                           | int64      | FALSE    |
| region                   | 50    | []                                                           | int64      | FALSE    |
| debt_loan_ratio          | 5822  | []                                                           | float64    | FALSE    |
| del_in_18month           | 12    | [0, 3, 1, 4, 2, 9, 5, 6, 7, 8, 10, 15]                       | int64      | FALSE    |
| scoring_low              | 132   | []                                                           | float64    | FALSE    |
| scoring_high             | 360   | []                                                           | float64    | FALSE    |
| known_outstanding_loan   | 47    | []                                                           | int64      | FALSE    |
| known_dero               | 13    | [1, 0, 2, 3, 8, 10, 4, 9, 5, 6, 11, 12, 7]                   | int64      | FALSE    |
| pub_dero_bankrup         | 5     | [1.0, 0.0, 3.0, 2.0, nan, 5.0]                               | float64    | **TRUE** |
| recircle_b               | 8558  | []                                                           | float64    | FALSE    |
| recircle_u               | 2991  | []                                                           | float64    | FALSE    |
| initial_list_status      | 2     | [1, 0]                                                       | int64      | FALSE    |
| app_type                 | 2     | [0, 1]                                                       | int64      | FALSE    |
| earlies_credit_mon       | 520   | []                                                           | **object** | FALSE    |
| title                    | 843   | []                                                           | int64      | FALSE    |
| policy_code              | 1     | [1]                                                          | int64      | FALSE    |
| f0                       | 29    | [3.0, 6.0, 4.0, 16.0, 5.0, 12.0, 0.0, 8.0, 14.0, 2.0, nan, 11.0, 10.0,  18.0, 26.0, 1.0, 7.0, 9.0, 13.0, 15.0, 20.0, 17.0, 19.0, 23.0, 21.0, 22.0,  33.0, 24.0, 25.0, 28.0] | float64    | **TRUE** |
| f1                       | 2     | [0.0, nan, 1.0]                                              | float64    | **TRUE** |
| f2                       | 58    | []                                                           | float64    | **TRUE** |
| f3                       | 62    | []                                                           | float64    | **TRUE** |
| f4                       | 39    | []                                                           | float64    | **TRUE** |
| early_return             | 6     | [0, 3, 1, 2, 5, 4]                                           | int64      | FALSE    |
| early_return_amount      | 3967  | []                                                           | int64      | FALSE    |
| early_return_amount_3mon | 3484  | []                                                           | float64    | FALSE    |
| isDefault                | 2     | [0, 1]                                                       | int64      | FALSE    |

åŸºäºå¦‚ä¸Šçš„æ•°æ®è¡¨æ ¼ï¼Œä¸å®éªŒæŒ‡å¯¼ä¹¦æ‰€æè¿°ï¼Œ`isDefault` ä¸º`y`ã€‚è€Œå‰©ä½™çš„å…¶ä»–ç‰¹å¾ä¸º`X`ã€‚

åŸºäºå¦‚ä¸Šçš„æ•°æ®è¡¨æ ¼ï¼Œéƒ¨åˆ†å±æ€§å­˜åœ¨ç¼ºå¤±ï¼Œåœ¨è¿›è¡Œè´å¶æ–¯é¢„æµ‹å‰éœ€è¦å¯¹ç©ºç™½æ•°æ®è¿›è¡Œå¡«å……ï¼Œç”±äºä¸ç¡®å®šæµ‹è¯•æ•°æ®æ˜¯å¦å’Œè®­ç»ƒæ•°æ®ä¿æŒä¸€æ ·çš„ç©ºå€¼åˆ†å¸ƒï¼Œå› æ­¤éœ€è¦è¿›è¡Œå¡«å……ï¼Œåœ¨ç¬¬äºŒéƒ¨åˆ†ä¼šé˜è¿°è¿™ä¸€éƒ¨åˆ†ã€‚

åŸºäºå¦‚ä¸Šçš„æ•°æ®è¡¨æ ¼ï¼Œéƒ¨åˆ†æ•°æ®å¹¶éä¸ºæ•°å€¼å‹ï¼Œä¾‹å¦‚æ–‡æœ¬æè¿°å’Œæ—¥æœŸæ ¼å¼ç­‰çº¯æ–‡æœ¬å†…å®¹ï¼Œéœ€è¦é€šè¿‡æŸç§æ˜ å°„æ–¹å¼æ¥å¯¹æ–‡æœ¬å‹æ•°æ®è¿›è¡Œè½¬æ¢ã€‚æ­¤å¤„å‡è®¾æµ‹è¯•æ•°æ®ä¸­ä¸å­˜åœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„ç‰¹å¾å€¼ï¼Œåœ¨ç¬¬ä¸‰éƒ¨åˆ†ä¼šé˜è¿°è¿™ä¸€éƒ¨åˆ†ã€‚

åŸºäºå¦‚ä¸Šçš„æ•°æ®è¡¨æ ¼ï¼Œéƒ¨åˆ†æ•°æ®ä¹‹é—´çš„å…³è”æ€§è¾ƒé«˜ï¼Œå¹¶ä¸”éƒ¨åˆ†æ•°æ®å…·æœ‰å”¯ä¸€æ€§æˆ–è€…ç‹¬ç«‹æ€§ï¼Œè¿™ç§æ•°æ®éƒ½å¯ä»¥åˆ é™¤ã€‚å¯¹å„åˆ—ä¹‹é—´è¿›è¡Œçº¿æ€§ç›¸å…³æ€§åˆ¤å®šï¼Œæ­¤å¤„å‡è®¾ä¸¤ä¸ªåˆ—å¦‚æœæ˜¯å¼ºç›¸å…³ï¼Œåˆ™åˆ å»å…¶ä¸­ä¸€åˆ—å³å¯ï¼Œåœ¨ç¬¬å››éƒ¨åˆ†ä¼šé˜è¿°è¿™ä¸€éƒ¨åˆ†ã€‚

åŸºäºå¦‚ä¸Šçš„æ•°æ®è¡¨æ ¼ï¼Œéƒ¨åˆ†æ•°æ®ä¸ºæ•°å€¼è¾ƒå¤šçš„è¿ç»­å€¼ã€‚å¯¹äºæ•°å€¼è¾ƒå°‘çš„è¿ç»­å€¼åœ¨è®¡ç®—ä¼¼ç„¶æ—¶å¯ä»¥ç®€å•çš„è®¡ç®—ï¼Œè€Œå¯¹äºæ•°å€¼è¾ƒå¤šçš„è¿ç»­å€¼è®¡ç®—ä¼¼ç„¶æ¯”è¾ƒå¤æ‚ï¼Œå› æ­¤éœ€è¦å¯¹æ•°æ®è¿›è¡Œåˆ†ç®±å¤„ç†ï¼Œæœ¬æ¬¡å®éªŒå°†ä¼šå…ˆç­›é€‰éœ€è¦åˆ†ç®±çš„æ•°æ®ï¼Œå¹¶ä¸”å…ˆè¿›è¡Œç®€å•çš„åˆ†ç®±ï¼Œéšåå†å¯¹å„åˆ†ç®±çš„ç®±æ•°è¿›è¡Œè°ƒæ•´æ¥æé«˜åˆ¤åˆ«çš„å‡†ç¡®ç‡ï¼ˆè™½ç„¶å¯èƒ½æ„ä¹‰ä¸å¤§ï¼‰ï¼Œåœ¨ç¬¬äº”éƒ¨åˆ†ä¼šé˜è¿°ç›¸å…³å†…å®¹ã€‚

å¯¹äºæ•´ä¸ªæ•°æ®å¤„ç†çš„è¿‡ç¨‹ï¼Œæˆ‘å¯¹å…¶è¿›è¡Œäº†åŒ…è£…ï¼Œå°†å…¶åŒ…è£…æˆä¸ºä¸€ä¸ªæ•°æ®å¤„ç†å™¨çš„ç±»ï¼ˆ`DataProcess`ï¼‰æ¥å®ç°æµ‹è¯•æ•°æ®å’Œè®­ç»ƒæ•°æ®çš„æ•°æ®å¤„ç†ä¸€è‡´æ€§ã€‚è¯¥ç±»ä¸­å„ä¸ªå¤„ç†éƒ¨åˆ†é‡‡ç”¨æµæ°´çº¿å·¥ä½œï¼Œå¤„ç†éƒ¨åˆ†ä¼šæ‰§è¡Œå¦‚ä¸Šçš„å››ä¸ªéƒ¨åˆ†ï¼Œå³å¡«å……ã€è½¬æ¢ã€åˆ é™¤ã€åˆ†ç®±ã€‚

### 2.æ•°æ®å¡«å……

åœ¨é€šå¸¸æƒ…å†µä¸‹æ•°æ®å¡«å……å­˜åœ¨å¤šç§å¡«å……æ–¹å¼ï¼Œæ¯”å¦‚ä¼—æ•°å¡«å……ã€ä¸­ä½æ•°å¡«å……ã€å¹³å‡å€¼å¡«å……ã€æŒ‡å®šå€¼å¡«å……ã€‚ä¸ºäº†æ›´å¥½çš„ç¼–è¾‘å’Œè°ƒè¯•ä»£ç ï¼Œæˆ‘è®¾è®¡è¯¥ç±»åœ¨åˆ›å»ºæ—¶ä¼šä¼ å…¥ä¸€ä¸ª`fillNanRules` åˆ—è¡¨æ¥è§„èŒƒçº¦æŸæ•°æ®å¡«å……çš„è¿‡ç¨‹ã€‚åœ¨è¯¥ç±»ä¸­å®ç°`fillNanMethod` æ–¹æ³•æ—¢å¯ä»¥æ ¹æ®åˆå§‹åŒ–çš„æ•°æ®å¡«å……è§„åˆ™è¿›è¡Œå¡«å……ã€‚

ä»¥ä¸‹ä¸ºæ•°æ®å¡«å……çš„åŸºæœ¬æ–¹æ³•ã€‚0ä»£è¡¨å¡«å……ä¼—æ•°ã€1ä»£è¡¨å¡«å……ä¸­ä½æ•°ã€2ä»£è¡¨å¹³å‡æ•°ã€3ä¸ºå¡«å……å…·ä½“å€¼ã€‚é»˜è®¤é‡‡ç”¨ä¼—æ•°å¡«å……ã€‚

```python
    def __init__(self, dataFrame,segmentMap, fillNanRules={}):
        '''
        :param segmentMap: åˆ†æ®µåˆ†ç®±åˆ—è¡¨
        :param fillNanRules: å¡«å……è§„åˆ™ name:[method,defaultValue]è¿™æ ·çš„å­—å…¸
        '''
        self.data = dataFrame
        self.fillNanRules = fillNanRules
        # ... å…¶ä»–å†…å®¹
        
    def __fillNan(self, column, typ=0, default_value=0):
        '''
        :param typ: 0 ä¸ºä¼—æ•°ã€1ä¸ºä¸­ä½æ•°ã€2ä¸ºå¹³å‡æ•°ã€3ä¸ºå¡«å……ä¸ºdefault_valueå€¼
        '''
        if typ not in [0, 1, 2, 3]:
            raise ValueError("type must be 0, 1, 2, or 3.")
        if typ == 0:
            column.fillna(column.mode()[0], inplace=True)
        if typ == 1:
            column.fillna(column.median(), inplace=True)
        if typ == 2:
            column.fillna(column.mean(), inplace=True)
        if typ == 3:
            column.fillna(default_value, inplace=True)
        return column
    
    def fillNanMethod(self, data):
        # é»˜è®¤å‡é‡‡ç”¨ä¼—æ•°æ¥å¡«å……æ‰€æœ‰ç¼ºå¤±å€¼
        for columnName in data.columns:
            if columnName in self.fillNanRules:
                if self.fillNanRules[columnName][0] == 3:
                    data[columnName] = self.__fillNan(data[columnName], self.fillNanRules[columnName][0],
                                                      self.fillNanRules[columnName][1])
                else:
                    data[columnName] = self.__fillNan(data[columnName], self.fillNanRules[columnName][0])
            else:
                data[columnName] = self.__fillNan(data[columnName])
        return data
```

### 3.æ•°æ®è½¬æ¢

é€šè¿‡å¯¹ä¸Šè¿°æ•°æ®è¡¨æ ¼çš„æŸ¥é˜…ï¼Œå¯ä»¥å¾—çŸ¥å¦‚ä¸‹è¡¨çš„éœ€è¦è¿›è¡Œæ•°æ®è½¬æ¢çš„ç‰¹å¾å€¼åˆ—è¡¨ï¼Œå°†å¯¹å…¶é€æ­¥è¿›è¡Œå¤„ç†ã€‚

| columnName         | count | unique                                                       | type   | has_null |
| ------------------ | ----- | ------------------------------------------------------------ | ------ | -------- |
| class              | 7     | ['B', 'C', 'A', 'G', 'D', 'E', 'F']                          | object | FALSE    |
| employer_type      | 6     | ['å¹¼æ•™ä¸ä¸­å°å­¦æ ¡', 'æ”¿åºœæœºæ„', 'ä¸Šå¸‚ä¼ä¸š', 'æ™®é€šä¼ä¸š', 'ä¸–ç•Œäº”ç™¾å¼º', 'é«˜ç­‰æ•™è‚²æœºæ„'] | object | FALSE    |
| industry           | 14    | ['é‡‘èä¸š', 'å›½é™…ç»„ç»‡', 'æ–‡åŒ–å’Œä½“è‚²ä¸š', 'å»ºç­‘ä¸š', 'ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§ä¾›åº”ä¸š', 'æ‰¹å‘å’Œé›¶å”®ä¸š', 'é‡‡çŸ¿ä¸š', 'æˆ¿åœ°äº§ä¸š',  'äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š', 'å…¬å…±æœåŠ¡ã€ç¤¾ä¼šç»„ç»‡', 'ä½å®¿å’Œé¤é¥®ä¸š', 'ä¿¡æ¯ä¼ è¾“ã€è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š', 'å†œã€æ—ã€ç‰§ã€æ¸”ä¸š', 'åˆ¶é€ ä¸š'] | object | FALSE    |
| work_year          | 11    | [nan, '8 years', '10+ years', '3 years', '7 years', '4 years', '1 year',  '< 1 year', '6 years', '2 years', '9 years', '5 years'] | object | TRUE     |
| issue_date         | 126   | []ï¼ˆæ—¥æœŸæ ¼å¼ yyyy/mm/ddï¼‰                                    | object | FALSE    |
| earlies_credit_mon | 520   | [] ï¼ˆæ—¥æœŸæ ¼å¼ï¼Œå…¶ä¸­å­˜åœ¨ä¸€ä¸ªè‹±æ–‡æœˆä»½ç¼©å†™å’Œä¸€äº›æ•°å­—ä»¥åŠå…¶ä»–å­—ç¬¦ï¼‰ | object | FALSE    |

è¯¥è¿‡ç¨‹çš„æ•´ä½“è°ƒç”¨å‡½æ•°å¦‚ä¸‹ï¼šéšåå°†åˆ†ç±»å‹æ¥é˜è¿°è¯¥è¿‡ç¨‹ã€‚

```python
    def dealStringMethod(self, data):
    	# ç¬¬ä¸€æ­¥å¤„ç† class å±æ€§
        data['class'] = self.__dealString(data['class'], self.Mapping['class'])
        # ç¬¬äºŒæ­¥å¤„ç† employer_type å±æ€§
        data['employer_type'] = self.__dealString(data['employer_type'], self.Mapping['employer_type'])
        # ç¬¬ä¸‰æ­¥å¤„ç† industry å±æ€§
        data['industry'] = self.__dealString(data['industry'], self.Mapping['industry'])
        # ç¬¬å››æ­¥å¤„ç† work_year å±æ€§
        data['work_year'] = self.__dealString(data['work_year'], self.Mapping['work_year'])
        
        # ç¬¬äº”æ­¥å¤„ç† issue_date å±æ€§ï¼Œè¯¥å±æ€§çš„å«ä¹‰ è´·æ¬¾å‘æ”¾çš„æœˆä»½,å› æ­¤åªç”¨æˆªè‡³åˆ°å¹´ä»½
        data['issue_date'] = self.__dealDate2Month(data['issue_date'])
        # ç¬¬å…­æ­¥å¤„ç† earlies_credit_mon å±æ€§ï¼Œè¯¥å±æ€§çº¦æŸåˆ°æœˆä»½
        data['earlies_credit_mon'] = self.__dealString2Month(data['earlies_credit_mon'])
        return data
```



#### 3.1 æ–‡æœ¬ç±»å‹è½¬æ¢ä¸ºæ•°å€¼ç±»å‹

å¯¹äºè¡¨æ ¼ä¸­çš„å‰å››è¡Œï¼Œå…¶ç‰¹ç‚¹å‡ä¸ºä¸€ä¸ªçº¯å­—ç¬¦ä¸²ç±»å‹çš„æ–‡æœ¬ï¼Œå¯ä»¥é€šè¿‡å†™ä¸€ä¸ªæ˜ å°„å…³ç³»æ¥å¯¹æ–‡æœ¬è½¬æ¢æˆæ•°å€¼ã€‚Mapæ˜ å°„å¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼Œåœ¨å®šä¹‰å¥½Mapæ˜ å°„åå¯ä»¥é€šè¿‡å®šä¹‰å¥½çš„å‡½æ•°æ¥æ‰§è¡Œ`__dealString`ï¼Œæ¥å¯¹æ•°æ®è¿›è¡Œå¡«å……ã€‚æ•´ä½“å®ç°è¿‡ç¨‹å¦‚ä¸‹ï¼š

```python
    Mapping = {'class': {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6},
               'employer_type': {'å¹¼æ•™ä¸ä¸­å°å­¦æ ¡': 0, 'æ”¿åºœæœºæ„': 1, 'ä¸Šå¸‚ä¼ä¸š': 2, 'æ™®é€šä¼ä¸š': 3, 'ä¸–ç•Œäº”ç™¾å¼º': 4,
                                 'é«˜ç­‰æ•™è‚²æœºæ„': 5},
               'industry': {'é‡‘èä¸š': 0, 'å›½é™…ç»„ç»‡': 1, 'æ–‡åŒ–å’Œä½“è‚²ä¸š': 2, 'å»ºç­‘ä¸š': 3, 'ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§ä¾›åº”ä¸š': 4,
                            'æ‰¹å‘å’Œé›¶å”®ä¸š': 5, 'é‡‡çŸ¿ä¸š': 6,
                            'æˆ¿åœ°äº§ä¸š': 7, 'äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š': 8, 'å…¬å…±æœåŠ¡ã€ç¤¾ä¼šç»„ç»‡': 9, 'ä½å®¿å’Œé¤é¥®ä¸š': 10,
                            'ä¿¡æ¯ä¼ è¾“ã€è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š': 11, 'å†œã€æ—ã€ç‰§ã€æ¸”ä¸š': 12, 'åˆ¶é€ ä¸š': 13},
               'work_year': {'8 years': 8, '10+ years': 10, '3 years': 3, '7 years': 7, '4 years': 4, '1 year': 1,
                             '< 1 year': 0, '6 years': 6, '2 years': 2, '9 years': 9, '5 years': 5}}

    @staticmethod
    def __dealString(column, mapping):
        values = column.values.tolist()
        for value in values:
            if value not in mapping:
                raise ValueError(f"Value '{value}' is not in {list(mapping.keys())}")
        column = column.map(mapping)
        return column
```

#### 3.2 ç®€å•æ—¥æœŸç±»å‹è½¬æ¢ä¸ºæ•°å€¼ç±»å‹

å¯¹äºè¡¨æ ¼ä¸­çš„ç¬¬äº”è¡Œ `issue_date` ï¼Œå…¶ä¸­çš„æ•°å€¼ç±»å‹ä¸º `yyyy/mm/dd` ï¼Œé€šè¿‡å®éªŒæŒ‡å¯¼ä¹¦ä¸­  `issue_date` ä¸ºè´·æ¬¾å‘æ”¾çš„æœˆä»½ï¼Œå› æ­¤åªéœ€è¦ç»Ÿè®¡åˆ°æœˆä»½çš„å·®å€¼å³å¯ã€‚åˆ©ç”¨pythonå†…éƒ¨è‡ªå¸¦çš„å¤„ç†æ–¹æ³•å³å¯ã€‚è®¡ç®—1970å¹´1æœˆåˆ°ç°åœ¨çš„æœˆä»½æ•°ï¼Œå³å¯å°†è¯¥åˆ—è½¬æ¢æˆæ•°å€¼ç±»å‹ã€‚

```python
    def __dealDate2Month(self, column):
        column = pd.to_datetime(column, format='%Y/%m/%d')
        column = column.apply(lambda x: self.__calcMon2Target(x, 1970, 1))
        return column
```

#### 3.3 ç¹ççš„æ—¥æœŸç±»å‹è½¬æ¢ä¸ºæ•°å€¼ç±»å‹

å¯¹äºè¡¨æ ¼ä¸­çš„ç¬¬å…­è¡Œæ•°æ® `earlies_credit_mon` ï¼Œå…¶å†…éƒ¨çš„å«ä¹‰ä¸ºå€Ÿæ¬¾äººæœ€æ—©æŠ¥å‘Šçš„ä¿¡ç”¨é¢åº¦å¼€ç«‹çš„æœˆä»½ã€‚å¯ä»¥å‘ç°è¯¥ç±»å‹çš„æ•°æ®æ„é€ ä¸ºä¸€ä¸ªè‹±æ–‡æœˆä»½ç¼©å†™å’Œä¸€äº›æ•°å­—ä»¥åŠå…¶ä»–å­—ç¬¦ï¼Œå¯ä»¥é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼çš„æ–¹æ³•å°†è¯¥æœˆä»½æå–å¤„ç†ï¼Œå¹¶è¿›è¡Œè‹±æ–‡ç¼©å†™å’Œæœˆä»½çš„è½¬æ¢ï¼Œå³å¯è·å¾—å¯¹åº”çš„æœˆä»½çš„æ•°å€¼ã€‚

```python
    def __dealString2Month(self, column):
        column = column.apply(lambda x: self.matchMonth(x))
        return column

    @staticmethod
    def matchMonth(text):
        pattern = re.compile(r'[A-Za-z]+')
        matches = pattern.findall(text)
        try:
            month = datetime.strptime(matches[0], "%b").month
        except ValueError:
            raise ValueError(f"Value '{matches[0]}' is a valid month")
        return month
```

è‡³æ­¤ï¼Œå…­ä¸ªä¸ä¸ºæ•°å€¼å‹çš„ç‰¹å¾éƒ½å·²ç»è¢«å¤„ç†å¥½äº†ï¼Œå¯ä»¥å¯¹æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†æä¸å¤„ç†ã€‚

### 4.æ•°æ®åˆ å‡

æ ¹æ®ä¸Šè¿°è¡¨æ ¼ä¸­çš„æ•°æ®å…ˆè¿›è¡Œå”¯ä¸€æ€§ï¼ˆå¯èƒ½å–å€¼ä»…æœ‰ä¸€ä¸ªï¼‰æˆ–è€…ç‹¬ç«‹æ€§ï¼ˆæ²¡ä¸ªæ•°æ®çš„å€¼å‡ä¸€æ ·ï¼‰åˆ¤æ–­ï¼Œå¯ä»¥å¾—åˆ°ä¸‹è¡¨ï¼š

| columnName  | count | unique | dtype | has_null |
| ----------- | ----- | ------ | ----- | -------- |
| loan_id     | 9000  | []     | int64 | FALSE    |
| user_id     | 9000  | []     | int64 | FALSE    |
| policy_code | 1     | [1]    | int64 | FALSE    |

åœ¨æœ¬å®éªŒä¸­ï¼Œå‡å®šå„ä¸ªç‰¹å¾æ˜¯åŸºæœ¬ç‹¬ç«‹çš„ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
$$
p(w|x) = \frac{p(w)p(x|w)}{p(x)}=\frac{p(w)\Pi_{i}\frac{n(i)}{n(w)}}{p(x)}
$$


é‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸€ä¸ªä¸åŒçš„æ•°æ®å–åŒä¸€ä¸ªå€¼æˆ–è€…å‡å–ä¸åŒçš„å€¼ï¼Œå¯¹æœ€åçš„æ•´ä½“ç­”æ¡ˆæ²¡æœ‰ä»»ä½•åŒºåˆ«ï¼Œå› æ­¤ä¸ºäº†è®¡ç®—çš„ä¾¿æ·ï¼Œåˆ å»è¿™ä¸‰è¡Œã€‚

è´å¶æ–¯æ¨¡å‹é€šå¸¸æ¶‰åŠåˆ°æ¦‚ç‡åˆ†å¸ƒçš„ä¼°è®¡å’Œä½¿ç”¨å…ˆéªŒçŸ¥è¯†ã€‚å¦‚æœä¸¤ä¸ªç‰¹å¾å¼ºç›¸å…³ï¼Œå®ƒä»¬å¯èƒ½ä¼šåœ¨æ¦‚ç‡ä¼°è®¡ä¸­å¼•å…¥å†—ä½™ï¼Œè¿™å¯èƒ½å¯¼è‡´å…ˆéªŒæˆ–ä¼¼ç„¶ä¼°è®¡ä¸å‡†ç¡®ã€‚å› æ­¤ï¼Œåœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œç§»é™¤ä¸€ä¸ªç‰¹å¾å¯èƒ½æœ‰åŠ©äºæ›´å‡†ç¡®åœ°ä¼°è®¡æ¨¡å‹å‚æ•°ã€‚

é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¢åŠ äº†æ•°æ®çš„ç»´åº¦ä½†æ²¡æœ‰ç›¸åº”å¢åŠ ä¿¡æ¯é‡ï¼Œè¿™å¯èƒ½ä½¿æ¨¡å‹æ›´éš¾ä»¥ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶å¢åŠ è¿‡æ‹Ÿåˆçš„é£é™©ã€‚å‡å°‘ç‰¹å¾çš„æ•°é‡å¯ä»¥å¸®åŠ©ç¼“è§£ç»´åº¦çš„è¯…å’’ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç‚¹è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚

è´å¶æ–¯æ¨¡å‹çš„è®¡ç®—é€šå¸¸æ¯”è¾ƒå¤æ‚å’Œæ˜‚è´µã€‚å‡å°‘ç‰¹å¾æ•°é‡å¯ä»¥é™ä½è®¡ç®—è´Ÿæ‹…ï¼Œæé«˜æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹çš„æ•ˆç‡ã€‚

åŸºäºä»¥ä¸Šä¸‰ç‚¹æˆ‘å¯¹æ•´ä¸ªè®­ç»ƒé›† `X` ä¸­çš„æ•°æ®è¿›è¡Œä»»æ„ä¸¤åˆ—çš„ç›¸å…³æ€§åˆ†æï¼Œè®¾è®¡å¦‚ä¸‹çš„ä»£ç ç”Ÿæˆå…¶å„åˆ—ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µï¼ˆä¸ºäº†ä¾¿äºå¤„ç†å·²ç»å¯¹å…¶å…¨éƒ¨è¿›è¡Œç»å¯¹å€¼å¤„ç†ï¼‰ã€‚

```python
correlation_matrix = df.corr()
correlation_matrix = correlation_matrix.abs()
correlation_matrix.to_excel('../data/ç›¸å…³ç³»æ•°çŸ©é˜µ.xlsx', sheet_name='Correlation Matrix')
```

 å¾—åˆ°çš„ç»“æœåœ¨ `Exp1/data/ç›¸å…³ç³»æ•°çŸ©é˜µ.xlsx` ä¹‹ä¸­ï¼Œåœ¨è¯¥ç›¸å…³ç³»æ•°çŸ©é˜µçš„åŸºç¡€ä¸Šï¼Œåˆ†æä»»æ„ä¸¤åˆ—ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç¼–å†™å¦‚ä¸‹ä»£ç æ£€æŸ¥å…¶ç›¸å…³ç³»æ•°çš„ç»å¯¹å€¼å¤§äº0.75çš„åˆ—ç»„åˆã€‚åˆ©ç”¨å¦‚ä¸‹ä»£ç ï¼Œå¾—åˆ°å¦‚ä¸‹è¡¨ã€‚

```python
def check_correlation_matrix():
    correlation_matrix = df.corr().abs()

    correlation_matrix_absolute = correlation_matrix.abs()

    # è·å–ä¸Šä¸‰è§’çŸ©é˜µï¼Œå¿½ç•¥å¯¹è§’çº¿
    upper_tri = np.triu(np.ones(correlation_matrix_absolute.shape))

    # ä»ä¸Šä¸‰è§’çŸ©é˜µä¸­ç­›é€‰å‡ºæ‰€æœ‰å¤§äºç­‰äº0.7å¹¶ä¸”å°äº1çš„å…ƒç´ çš„ä½ç½®
    to_select = (correlation_matrix_absolute >= 0.7) & (correlation_matrix_absolute < 1) & upper_tri

    # è·å–æ»¡è¶³æ¡ä»¶çš„è¡Œåä¸åˆ—åçš„ç»„åˆ
    selected_correlations = correlation_matrix_absolute.where(to_select).stack()

    print("Selected Column Pairs with Correlation >= 0.7 and < 1 and Their Correlations:")
    for (pair, correlation) in selected_correlations.items():
        print(f"{pair}: {correlation}")

check_correlation_matrix()
```

| åˆ—1                   | åˆ—2                        | ç›¸å…³ç³»æ•°ç»å¯¹å€¼(4ä½æœ‰æ•ˆæ•°å­—) | æ€è€ƒ                                                         |
| :-------------------- | -------------------------- | --------------------------- | ------------------------------------------------------------ |
| `interest`            | `class`                    | 0.9254                      | `interest` è¡¨ç¤ºå½“å‰è´·æ¬¾åˆ©ç‡ä¸ºè¿ç»­çš„ï¼Œè€Œ`class` è¡¨ç¤ºè´·æ¬¾çº§åˆ«ä¸ºç¦»æ•£çš„ï¼Œä»ç°å®ç”Ÿæ´»ä¸­è€ƒè™‘è¿™ä¸¤è€…ä¹‹é—´ç†åº”å­˜åœ¨æŸäº›è”ç³»ï¼Œå¹¶ä¸”åœ¨`interest` è¿›è¡Œåˆ†ç®±åçš„æ•ˆæœä¸ä¸€å®šæ¯”ç›´æ¥é‡‡ç”¨`class` è¿™ç§ç¦»æ•£å€¼æ–¹ä¾¿ï¼Œå¯ä»¥ç»˜åˆ¶ä¸¤è€…çš„ç›´æ–¹å›¾è¿›è¡Œæ¯”è¾ƒï¼Œå°†`interest` è¿›è¡Œç»†åˆ†åå¤§è‡´åˆ†å¸ƒå½¢å¼åº”ä¸`class` ç›¸ä¼¼ï¼Œæœ¬æ¬¡å®éªŒä¸­å°†`interest` åˆ é™¤ã€‚ç›´æ–¹å›¾å¦‚ä¸‹å›¾ã€‚ |
| `f3`                  | `f4`                       | 0.8438                      | è¿™ä¸¤ä¸ªå±æ€§å‡ä¸ºä¸ºä¸€äº›è´·æ¬¾äººè¡Œä¸ºè®¡æ•°ç‰¹å¾çš„å¤„ç†ï¼Œä¸¤ä¸ªå€¼ä¸ä¸ºç¦»æ•£çš„ï¼Œæˆ‘åœ¨æœ¬å®éªŒä¸­é€‰å–æ•°æ®è¾ƒå¤šçš„`f3`ï¼Œåˆ é™¤`f4`,ä¸¤è€…çš„ç›´æ–¹å›¾å¦‚ä¸‹ã€‚ |
| `early_return_amount` | `early_return_amount_3mon` | 0.7530                      | `early_return_amount`è´·æ¬¾äººæå‰è¿˜æ¬¾ç´¯ç§¯é‡‘é¢ï¼Œ`early_return_amount_3mon`è¿‘3ä¸ªæœˆå†…æå‰è¿˜æ¬¾é‡‘é¢ï¼Œä¸¤ä¸ªå€¼ä¸ä¸ºç¦»æ•£çš„ï¼Œæˆ‘åœ¨æœ¬å®éªŒä¸­é€‰å–æ•°æ®è¾ƒå¤šçš„`early_return_amount_3mon`ï¼Œåˆ é™¤`early_return_amount`ï¼Œæ•°æ®æåº¦åæ€ï¼Œä¸å†å±•ç¤ºç›´æ–¹å›¾ã€‚ |
| `scoring_low`         | `scoring_high`             | 0.8890                      | `scoring_low `  è¡¨ç¤ºå€Ÿæ¬¾äººåœ¨è´·æ¬¾è¯„åˆ†ä¸­æ‰€å±çš„ä¸‹é™èŒƒå›´ï¼Œ`scoring_high` è¡¨ç¤ºå€Ÿæ¬¾äººåœ¨è´·æ¬¾è¯„åˆ†ä¸­æ‰€å±çš„ä¸Šé™èŒƒå›´ï¼Œä¸¤ä¸ªå€¼ä¸ä¸ºç¦»æ•£çš„ï¼Œæˆ‘åœ¨æœ¬å®éªŒä¸­é€‰å–æ•°æ®è¾ƒå¤šçš„`scoring_low `ï¼Œåˆ é™¤`scoring_high`,ä¸¤è€…çš„ç›´æ–¹å›¾å¦‚ä¸‹ã€‚ |
| `total_loan`          | `monthly_payment`          | 0.9256                      | `total_loan`  è¡¨ç¤ºè´·æ¬¾æ•°é¢ï¼Œ`monthly_payment` è¡¨ç¤ºåˆ†æœŸä»˜æ¬¾é‡‘é¢ï¼Œä¸¤ä¸ªå€¼ä¸ä¸ºç¦»æ•£çš„ï¼Œæˆ‘åœ¨æœ¬å®éªŒä¸­é€‰å–æ•°æ®è¾ƒå¤šçš„`scoring_low `ï¼Œåˆ é™¤`scoring_high`,ä¸¤è€…çš„ç›´æ–¹å›¾å¦‚ä¸‹ã€‚å¹¶ä¸”è€ƒè™‘å®é™…æƒ…å†µä¸‹`total_loan`ä¼šå†³å®š`monthly_payment`ï¼Œå¹¶ä¸”åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ä¼¼ä¹åè€…æ›´ä¼šå½±å“å€Ÿæ¬¾äººæ˜¯å¦è¿˜è´·ã€‚ |

`interest` å’Œ `class` çš„åˆ†å¸ƒç›´æ–¹å›¾ï¼š

![](G:\ExpMachineLearn\ExpML\Exp1\pic\class&interest_Bar.png)

`f3` å’Œ `f4` çš„åˆ†å¸ƒç›´æ–¹å›¾ï¼š

![](G:\ExpMachineLearn\ExpML\Exp1\pic\f3&f4_Bar.png)

`scoring_low` å’Œ `scoring_high`åˆ†å¸ƒç›´æ–¹å›¾ï¼š

![](G:\ExpMachineLearn\ExpML\Exp1\pic\scoring_low&scoring_high_Bar.png)

`total_loan` å’Œ `monthly_payment` åˆ†å¸ƒç›´æ–¹å›¾ï¼š

![](G:\ExpMachineLearn\ExpML\Exp1\pic\total_loan&monthly_payment_Bar.png)

### 5. æ•°æ®åˆ†ç®±

æ•°æ®åˆ†ç®±å¯ä»¥ç®€åŒ–æ•°æ®ç»“æ„ï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“ç†è§£å’Œå®ç°ã€‚é€šè¿‡å°†è¿ç»­çš„æ•°æ®å€¼åˆ†ç»„åˆ°æœ‰é™æ•°é‡çš„â€œç®±â€æˆ–â€œåŒºé—´â€ä¸­ï¼Œå¯ä»¥å‡å°‘æ•°æ®çš„å¤æ‚æ€§ï¼Œä»è€Œé™ä½æ¨¡å‹çš„è¿‡æ‹Ÿåˆé£é™©ã€‚

åˆ†ç®±å¯ä»¥æé«˜æ¨¡å‹å¯¹äºå¼‚å¸¸å€¼å’Œå™ªå£°æ•°æ®çš„é²æ£’æ€§ã€‚åŸå§‹æ•°æ®ä¸­çš„å°æ³¢åŠ¨å¯èƒ½ä¸ä¼šå¯¼è‡´åˆ†ç®±åçš„æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤æ¨¡å‹å¯¹äºè¾“å…¥æ•°æ®çš„å°å˜åŠ¨ä¸å¤ªæ•æ„Ÿã€‚

åœ¨è´å¶æ–¯å†³ç­–å’Œæ¦‚ç‡ä¼°è®¡ä¸­ï¼Œå¤„ç†è¾ƒå°‘çš„åˆ†ç±»å¯ä»¥ç®€åŒ–æ¦‚ç‡è®¡ç®—ã€‚åˆ†ç®±åï¼Œæ¯ä¸ªç®±ä¸­çš„è§‚æµ‹å€¼å¯ä»¥è¢«ç”¨æ¥ä¼°è®¡è¯¥åŒºé—´çš„æ¡ä»¶æ¦‚ç‡ï¼Œä»è€Œåœ¨è´å¶æ–¯å†³ç­–ä¸­æä¾›æœ‰ç”¨çš„å…ˆéªŒå’Œä¼¼ç„¶ä¿¡æ¯ã€‚

å¯¹äºæœ‰å¾ˆå¤šæ•°å€¼ç‰¹å¾çš„æ•°æ®é›†ï¼Œåˆ†ç®±å¯ä»¥æœ‰æ•ˆå‡å°‘æ¨¡å‹éœ€è¦å¤„ç†çš„æ•°æ®ç»´åº¦ã€‚è¿™ä¸ä»…å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œè¿˜å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…ç»´åº¦ç¾éš¾ã€‚

é€šå¸¸åˆ©ç”¨ç»éªŒå¯¹æ•°æ®è¿›è¡Œåˆ†ç®±çš„æ–¹æ³•å¦‚ä¸‹ï¼š

- **å¹³æ–¹æ ¹æ³•**ï¼šå°†åˆ†ç®±æ•°è®¾ä¸ºæ•°æ®ç‚¹æ€»æ•°çš„å¹³æ–¹æ ¹ã€‚è¿™æ˜¯ä¸€ç§ç®€å•ä¸”å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹å…¶ä»–ä¿¡æ¯æ—¶ã€‚
- **Sturges' è§„åˆ™**ï¼šè¿™ä¸ªå…¬å¼æ˜¯ $ğ‘˜=1+logâ¡_{2}ğ‘›$ï¼Œå…¶ä¸­ $n$ æ˜¯æ ·æœ¬æ•°é‡ã€‚è¿™ä¸ªè§„åˆ™åŸºäºæ•°æ®åˆ†å¸ƒæ˜¯è¿‘ä¼¼æ­£æ€çš„å‡è®¾ï¼Œä¸”ç›®çš„æ˜¯å°½å¯èƒ½å‡å°‘åœ¨ä¼°è®¡æ¦‚ç‡åˆ†å¸ƒæ—¶çš„æ€»æ–¹å·®ã€‚
- **Rice è§„åˆ™**ï¼šæè®®ä½¿ç”¨ $ğ‘˜=2Ã—ğ‘›^{1/3}$ ä½œä¸ºåˆ†ç®±ä¸ªæ•°ï¼Œè¿™ä¸ªæ–¹æ³•è¯•å›¾åœ¨ä¸å¢åŠ è¿‡å¤šç®±å­çš„å‰æä¸‹ï¼Œæä¾›è¶³å¤Ÿçš„ç»†è‡´åˆ’åˆ†ã€‚

åœ¨è¿™é‡Œä¸ºäº†ä¾¿äºåˆ†ç®±ï¼Œæˆ‘å…ˆåˆæ­¥é‡‡ç”¨ Sturges è§„åˆ™ï¼Œåˆ†ç®±æ•° $k = 1+log_{2}n = 14$ ã€‚å¯¹äºå–å€¼å¤§äº50ç§çš„ç‰¹å¾è¿›è¡Œåˆ†ç®±ï¼Œè€Œå¯¹äºç‰¹å¾å–å€¼æ•°å°äº50çš„ç‰¹å¾ä¿ç•™åŸæœ‰å½¢æ€ï¼Œæ¥ä¿è¯éƒ¨åˆ†æ•°æ®çš„åŸå§‹æ€§ï¼Œé˜²æ­¢åˆ†ç®±æ¯ä¸ªç®±å­å†…æ•°æ®è¿‡å¤šã€‚ 

é€šè¿‡å®šä¹‰ä¸åŒåç§°çš„åˆ†ç®±æ•°ï¼Œå¹¶åŒä¸€è°ƒç”¨å‡½æ•°è¿›è¡Œåˆ†ç®±ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ç§å°†åˆ†ç®±è¾¹ç•Œä¿å­˜èµ·æ¥ï¼Œä»¥ä¾¿åç»­å¤„ç†æµ‹è¯•æ•°æ®ï¼Œæ•´ä½“ä»£ç å¦‚ä¸‹ï¼š

```python
segmentMap = {
        'post_code': 14,
        'title': 14,
        'known_outstanding_loan': 14,
        'monthly_payment': 14,
        'issue_date': 14,
        'debt_loan_ratio': 14,
        'scoring_high': 14,
        'recircle_b': 14,
        'recircle_u': 14,
        'f0': 14,
        'f2': 14,
        'f3': 14,
        'early_return_amount': 14,
    }
    

    def dealSegment(self, data, typ='train'):
        for columnName in data.columns:
            data[columnName] = self.__dealDataSegment(data[columnName], columnName, typ=typ)
        return data
        
    def __dealDataSegment(self, column, column_name, typ='train'):
        if column_name in self.Segment:
            column = pd.cut(column, bins=self.Segment[column_name], labels=range(len(self.Segment[column_name]) - 1),
                            include_lowest=True)
            return column
        if typ == 'test':
            return column
        if column_name not in self.segmentMap:
            return column

        num = self.segmentMap[column_name]
        bins = np.linspace(column.min(), column.max(), num + 1)
        column = pd.cut(column, bins=bins, labels=range(num),
                        include_lowest=True)
        self.Segment[column_name] = bins
        return column
```

åœ¨æ­¤å…ˆè¿›è¡Œç®€å•çš„åˆ†ç®±å¤„ç†ï¼Œæ•´ä¸ªé¢„æµ‹æ¨¡å‹æ„å»ºå¥½ä¹‹åå†å¯¹åˆ†ç®±æ•°è¿›è¡Œä¼˜åŒ–ç»†åˆ†ã€‚



### 6.æ•°æ®å¤„ç†å™¨ä»£ç å®ç°

```python
'''
Exp1/code/DataProcess.py
'''


import re
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
from dateutil.relativedelta import relativedelta


class DataProcessor:
    def __init__(self, dataFrame,segmentMap, fillNanRules={}):
        '''
        :param segmentMap: åˆ†æ®µåˆ†ç®±åˆ—è¡¨
        :param fillNanRules: å¡«å……è§„åˆ™ name:[method,defaultValue]è¿™æ ·çš„å­—å…¸
        '''
        self.data = dataFrame
        self.fillNanRules = fillNanRules
        self.isProcessed = False
        self.segmentMap = segmentMap

    def __fillNan(self, column, typ=0, default_value=0):
        '''
        :param typ: 0 ä¸ºä¼—æ•°ã€1ä¸ºä¸­ä½æ•°ã€2ä¸ºå¹³å‡æ•°ã€3ä¸ºå¡«å……ä¸ºdefault_valueå€¼
        '''
        if typ not in [0, 1, 2, 3]:
            raise ValueError("type must be 0, 1, 2, or 3.")
        if typ == 0:
            column.fillna(column.mode()[0], inplace=True)
        if typ == 1:
            column.fillna(column.median(), inplace=True)
        if typ == 2:
            column.fillna(column.mean(), inplace=True)
        if typ == 3:
            column.fillna(default_value, inplace=True)
        return column

    @staticmethod
    def dropColumn(data):
        data = data.drop(['isDefault'], axis=1)

        # åˆ é™¤è‹¥å¹²åˆ—,ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¯ä»¥åˆæ­¥çœ‹å‡º
        # loan_idå’Œuser_idæ˜¯åŸºæœ¬ä¸é‡å¤çš„ï¼Œå› æ­¤å¯ä»¥åˆ å»ã€‚å½“å‰æ•°æ®ä¸‹policy_codeä¸‹å‡ä¸º1ï¼Œä¹Ÿå¯åˆ å»ã€‚
        data = data.drop(['loan_id'], axis=1)
        data = data.drop(['user_id'], axis=1)
        data = data.drop(['policy_code'], axis=1)

        # interest ä¸ classçš„å¼ºç›¸å…³
        # ('interest', 'class'): 0.9254597054479434
        data = data.drop(['interest'], axis=1)

        # ('f3', 'f4'): 0.8438089877232243
        data = data.drop(['f4'], axis=1)

        # ('early_return_amount', 'early_return_amount_3mon'): 0.7530913899047247
        data = data.drop(['early_return_amount'], axis=1)

        # ('scoring_low', 'scoring_high'): 0.8890661841570701
        data = data.drop(['scoring_low'], axis=1)

        # ('total_loan', 'monthly_payment'): 0.9256103360334527
        data = data.drop(['total_loan'], axis=1)
        return data

    def fillNanMethod(self, data):
        # é»˜è®¤å‡é‡‡ç”¨ä¼—æ•°æ¥å¡«å……æ‰€æœ‰ç¼ºå¤±å€¼
        for columnName in data.columns:
            if columnName in self.fillNanRules:
                if self.fillNanRules[columnName][0] == 3:
                    data[columnName] = self.__fillNan(data[columnName], self.fillNanRules[columnName][0],
                                                      self.fillNanRules[columnName][1])
                else:
                    data[columnName] = self.__fillNan(data[columnName], self.fillNanRules[columnName][0])
            else:
                data[columnName] = self.__fillNan(data[columnName])
        return data

    Mapping = {'class': {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6},
               'employer_type': {'å¹¼æ•™ä¸ä¸­å°å­¦æ ¡': 0, 'æ”¿åºœæœºæ„': 1, 'ä¸Šå¸‚ä¼ä¸š': 2, 'æ™®é€šä¼ä¸š': 3, 'ä¸–ç•Œäº”ç™¾å¼º': 4,
                                 'é«˜ç­‰æ•™è‚²æœºæ„': 5},
               'industry': {'é‡‘èä¸š': 0, 'å›½é™…ç»„ç»‡': 1, 'æ–‡åŒ–å’Œä½“è‚²ä¸š': 2, 'å»ºç­‘ä¸š': 3, 'ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§ä¾›åº”ä¸š': 4,
                            'æ‰¹å‘å’Œé›¶å”®ä¸š': 5, 'é‡‡çŸ¿ä¸š': 6,
                            'æˆ¿åœ°äº§ä¸š': 7, 'äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š': 8, 'å…¬å…±æœåŠ¡ã€ç¤¾ä¼šç»„ç»‡': 9, 'ä½å®¿å’Œé¤é¥®ä¸š': 10,
                            'ä¿¡æ¯ä¼ è¾“ã€è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š': 11, 'å†œã€æ—ã€ç‰§ã€æ¸”ä¸š': 12, 'åˆ¶é€ ä¸š': 13},
               'work_year': {'8 years': 8, '10+ years': 10, '3 years': 3, '7 years': 7, '4 years': 4, '1 year': 1,
                             '< 1 year': 0, '6 years': 6, '2 years': 2, '9 years': 9, '5 years': 5}}

    @staticmethod
    def __dealString(column, mapping):
        values = column.values.tolist()
        for value in values:
            if value not in mapping:
                raise ValueError(f"Value '{value}' is not in {list(mapping.keys())}")
        column = column.map(mapping)
        return column

    @staticmethod
    def __calcMon2Target(date, target_year, target_month):
        target_date = datetime(target_year, target_month, 1).date()
        diff = relativedelta(date, target_date)
        total_months = total_months = diff.years * 12 + diff.months
        return total_months

    def __dealDate2Month(self, column):
        column = pd.to_datetime(column, format='%Y/%m/%d')
        column = column.apply(lambda x: self.__calcMon2Target(x, 1970, 1))
        return column

    def __dealString2Month(self, column):
        column = column.apply(lambda x: self.matchMonth(x))
        return column

    @staticmethod
    def matchMonth(text):
        pattern = re.compile(r'[A-Za-z]+')
        matches = pattern.findall(text)
        try:
            month = datetime.strptime(matches[0], "%b").month
        except ValueError:
            raise ValueError(f"Value '{matches[0]}' is a valid month")
        return month

    def dealStringMethod(self, data):
        data['class'] = self.__dealString(data['class'], self.Mapping['class'])
        # ç¬¬äºŒæ­¥å¤„ç† employer_type å±æ€§
        data['employer_type'] = self.__dealString(data['employer_type'], self.Mapping['employer_type'])
        # ç¬¬ä¸‰æ­¥å¤„ç† industry å±æ€§
        data['industry'] = self.__dealString(data['industry'], self.Mapping['industry'])
        # ç¬¬å››æ­¥å¤„ç† work_year å±æ€§
        data['work_year'] = self.__dealString(data['work_year'], self.Mapping['work_year'])
        # ç¬¬äº”æ­¥å¤„ç† issue_date å±æ€§ï¼Œè¯¥å±æ€§çš„å«ä¹‰ è´·æ¬¾å‘æ”¾çš„æœˆä»½,å› æ­¤åªç”¨æˆªè‡³åˆ°å¹´ä»½
        data['issue_date'] = self.__dealDate2Month(data['issue_date'])
        # ç¬¬å…­æ­¥å¤„ç† earlies_credit_mon å±æ€§ï¼Œè¯¥å±æ€§çº¦æŸåˆ°æœˆä»½
        data['earlies_credit_mon'] = self.__dealString2Month(data['earlies_credit_mon'])
        return data

    @staticmethod
    def calculate_within_bin_variance(data, bins):
        hist, bin_edges = np.histogram(data, bins=bins)
        within_variances = []
        for i in range(len(bin_edges) - 1):
            bin_data = data[(data >= bin_edges[i]) & (data < bin_edges[i + 1])]
            if len(bin_data) > 1:
                within_variances.append(np.var(bin_data))
            else:
                within_variances.append(0)
        return np.mean(within_variances) if within_variances else float('inf')

    @staticmethod
    def calculate_between_bin_variance(data, bins):
        hist, bin_edges = np.histogram(data, bins=bins)
        bin_means = []
        for i in range(len(bin_edges) - 1):
            bin_data = data[(data >= bin_edges[i]) & (data < bin_edges[i + 1])]
            if len(bin_data) > 0:
                bin_means.append(np.mean(bin_data))
        overall_mean = np.mean(data)
        between_variance = sum(
            [(mean - overall_mean) ** 2 * len(data[(data >= bin_edges[i]) & (data < bin_edges[i + 1])]) for i, mean in
             enumerate(bin_means)])
        return between_variance / len(data)

    def find_optimal_bins(self, data, max_bins=90):
        best_bins = 2
        best_score = float('-inf')
        for bins in range(2, max_bins + 1):
            within_variance = self.calculate_within_bin_variance(data, bins)
            between_variance = self.calculate_between_bin_variance(data, bins)
            score = between_variance - within_variance  # Aim for low within and high between
            if score > best_score:
                best_score = score
                best_bins = bins
            # print(f"Bins: {bins}, Score: {score}, Within Variance: {within_variance}, Between Variance: {between_variance}")
        return best_bins

    Segment = {}

    def __dealDataSegment(self, column, column_name, typ='train'):
        if column_name in self.Segment:
            column = pd.cut(column, bins=self.Segment[column_name], labels=range(len(self.Segment[column_name]) - 1),
                            include_lowest=True)
            return column
        if typ == 'test':
            return column
        if column_name not in self.segmentMap:
            return column

        num = self.segmentMap[column_name]
        bins = np.linspace(column.min(), column.max(), num + 1)
        column = pd.cut(column, bins=bins, labels=range(num),
                        include_lowest=True)
        self.Segment[column_name] = bins
        return column

        # length = len(column.value_counts().index)
        # if 50 <= length <= 2500:
        #     num = 1 + int(np.log2(length))
        #     bins = np.linspace(column.min(), column.max(), num + 1)
        #     column = pd.cut(column, bins=bins, labels=range(num),
        #                     include_lowest=True)
        #     self.Segment[column_name] = bins
        #     return column
        # elif length > 2500:
        #     # num = int(length / 100)
        #     num = 2 * int(length ** (1 / 3))
        #     bins = np.linspace(column.min(), column.max(), num + 1)
        #     column = pd.cut(column, bins=bins, labels=range(num),
        #                     include_lowest=True)
        #     self.Segment[column_name] = bins
        #     return column
        # return column

    def dealSegment(self, data, typ='train'):
        for columnName in data.columns:
            data[columnName] = self.__dealDataSegment(data[columnName], columnName, typ=typ)
        return data

    def Process(self):
        self.isProcessed = True
        y_process = self.data['isDefault']
        self.data = self.dropColumn(self.data)
        self.data = self.fillNanMethod(self.data)
        self.data = self.dealStringMethod(self.data)
        self.data = self.dealSegment(self.data)
        X_process = self.data
        print("Process")
        return X_process, y_process

    def Deal(self, df):
        print("Deal For Test")
        if self.isProcessed is False:
            raise ValueError(f'This DataProcessor is not processed or trained')
        y_process = df['isDefault']
        # print("Test Drop")
        df = self.dropColumn(df)
        # print("Test Fill")
        df = self.fillNanMethod(df)
        # print("Test Trans")
        df = self.dealStringMethod(df)
        # print("Test Segment")
        df = self.dealSegment(df, 'test')
        X_process = df
        # print("Test Data Finished")
        return X_process, y_process
```



### 7.è´å¶æ–¯å†³ç­–å™¨å®ç°

#### 7.1 ç†è®ºç®€è¿°ä¸ç®€åŒ–

è´å¶æ–¯ç†è®ºçš„å…¬å¼å¦‚ä¸‹ï¼š
$$
p(w|x)=\frac{p(w)p(x|w)}{p(x)}
$$
å…¶ä¸­ï¼Œ$p(w|x)$ä¸ºåéªŒæ¦‚ç‡ï¼Œ$p(x|w)$ä¸ºç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦ï¼Œ$p(w)$ä¸ºå…ˆéªŒæ¦‚ç‡ã€‚æ˜¾ç„¶å½“è¾¹ç¼˜æ¦‚ç‡$p(x)$ç¡®å®šæ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—å¹¶æ¯”è¾ƒåˆ†å­çš„å¤§å°ï¼Œå³å¯å–å¾—æœ€å¤§çš„åéªŒæ¦‚ç‡ã€‚

åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå‡å®šå‰©ä½™çš„æ‰€æœ‰ç‰¹å¾å‡æ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆ$p(x|w)=\Pi p(x_{i}|w)= \Pi \frac{p(x_{i}Â·w)}{p(w)}= \Pi \frac{n(x_{i}Â·w)}{n(w)}$

å› æ­¤åœ¨å†³ç­–å™¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å°†å½“å‰å‡ºç°çš„æ‰€æœ‰äº§ç”Ÿçš„ç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦è¿›è¡Œä¿å­˜å³å¯ã€‚

å¯¹äºå…ˆéªŒæ¦‚ç‡ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç®€å•çš„è®¡æ•°è¿‡ç¨‹æ¥å®ç°ã€‚$p(w)=\frac{n(w)}{n}$ï¼Œå¹¶è¿›è¡Œä¿å­˜å³å¯ã€‚

#### 7.2 æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘

æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼ˆLaplace Smoothingï¼‰ï¼Œä¹Ÿç§°ä¸ºåŠ ä¸€å¹³æ»‘ï¼Œæ˜¯ä¸€ç§åœ¨æ¦‚ç‡ä¼°è®¡ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œå°¤å…¶åœ¨å¤„ç†åˆ†ç±»æ•°æ®æ—¶ï¼Œä¾‹å¦‚åœ¨æ–‡æœ¬åˆ†ç±»å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„è´å¶æ–¯æ¨¡å‹ä¸­ã€‚æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘çš„ä¸»è¦ç›®çš„æ˜¯è§£å†³é›¶æ¦‚ç‡é—®é¢˜ï¼Œç¡®ä¿åœ¨æ¨¡å‹ä¸­ä¸ä¼šå‡ºç°ä»»ä½•æ¦‚ç‡çš„ä¼°è®¡å€¼ä¸ºé›¶ã€‚

å½“åœ¨æµ‹è¯•æ•°æ®æˆ–å…¶ä»–æ•°æ®ä¸­æŸä¸ªç±»åˆ«çš„æŸä¸ªç‰¹å¾ä»æœªå‡ºç°è¿‡æ—¶ï¼ŒæŒ‰ç…§å¸¸è§„çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œè¿™ä¸ªç±»åˆ«ä¸‹è¿™ä¸ªç‰¹å¾çš„æ¦‚ç‡å°†ä¼šæ˜¯é›¶ã€‚è¿™ä¼šå¯¼è‡´æ•´ä¸ªæ•°æ®æ ·æœ¬çš„æ¦‚ç‡è®¡ç®—ç»“æœä¹Ÿä¸ºé›¶ï¼Œå½±å“æ¨¡å‹çš„é¢„æµ‹æ•ˆæœã€‚é€šè¿‡æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œå³ä½¿æ•°æ®åœ¨è®­ç»ƒé›†ä¸­æ²¡æœ‰å‡ºç°ï¼Œä¹Ÿå¯ä»¥èµ‹äºˆå®ƒä¸€ä¸ªå°çš„éé›¶æ¦‚ç‡ï¼Œä»è€Œé¿å…é›¶æ¦‚ç‡çš„é—®é¢˜ã€‚

æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘é€šè¿‡åœ¨è®¡æ•°ä¸­åŠ ä¸Šä¸€ä¸ªæ­£æ•°ï¼ˆé€šå¸¸æ˜¯1ï¼‰ï¼Œå¯¹æ¦‚ç‡ä¼°è®¡è¿›è¡Œå¹³æ»‘ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºæ•°æ®ç¨€ç–çš„æƒ…å†µï¼Œå¯ä»¥å‡å°‘ä¼°è®¡å€¼å¯¹äºæœªè§æ•°æ®çš„æ•æ„Ÿæ€§ã€‚åœ¨æ­¤å®éªŒä¸­è®¾ç½®ä¸€ä¸ªæ‹‰æ™®æ‹‰æ–¯ç³»æ•°$\alpha$ æ¥è¡¨ç¤ºè¿™ä¸ªæ­£æ•°ã€‚åœ¨æœªè§æ•°æ®ï¼ˆå³è®­ç»ƒæ•°æ®ä¸­æœªå‡ºç°çš„ç‰¹å¾ç»„åˆï¼‰ä¸Šï¼Œæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å¯ä»¥å¸®åŠ©æ¨¡å‹åšå‡ºæ›´åˆç†çš„é¢„æµ‹ï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ¤å®šè¿™äº›æƒ…å†µçš„æ¦‚ç‡ä¸ºé›¶ã€‚è¿™æœ‰åŠ©äºå¢å¼ºæ¨¡å‹å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚ 

æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å¯ä»¥çœ‹ä½œæ˜¯åœ¨è´å¶æ–¯æ¡†æ¶ä¸‹çš„å…ˆéªŒçŸ¥è¯†çš„å¼•å…¥ã€‚åœ¨æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ç¡®å®šæŸä¸ªäº‹ä»¶çš„æ¦‚ç‡æ—¶ï¼Œé»˜è®¤æ‰€æœ‰å¯èƒ½ç»“æœç­‰å¯èƒ½ï¼Œè¿™åæ˜ äº†ä¸€ç§éä¿¡æ¯å…ˆéªŒçš„æ€æƒ³ã€‚

å¯¹äºè®­ç»ƒè¿‡ç¨‹ä¸­ï¼š$y$ è¡¨ç¤ºåˆ†ç±»æ ‡ç­¾çš„æ‰€æœ‰é›†åˆï¼Œ$X_{i}$è¡¨ç¤º$x_{i}$æ‰€æœ‰å–å€¼çš„é›†åˆ

å¯¹äºå…ˆéªŒæ¦‚ç‡ï¼š$p(w)=\frac{n(w)}{n}\approx \frac{n(w)+\alpha}{n+\alphaÂ·total(y)}$

å¯¹äºç±»æ¡ä»¶æ¦‚ç‡å¯†åº¦ï¼š$p(x_{i}|w) = \frac{p(x_{i}Â·w)}{p(w)}= \frac{n(x_{i}Â·w)}{n(w)} \approx \frac{n(x_{i}Â·w)+\alpha}{n(w)+\alphaÂ·total(X_{i})}$

é‚£ä¹ˆå¯¹äºåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°çš„æœªåœ¨$X_{i}$ä¸­å‡ºç°çš„å€¼$x_{i}'$ï¼Œ$p(x_{i}|w) = \approx \frac{\alpha}{\alphaÂ·total(X_{i})}$

åŸºäºå¦‚ä¸Šçš„ç®€åŒ–å’Œæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œå³å¯ä»¥å†™å‡ºè´å¶æ–¯å†³ç­–çš„ä»£ç ã€‚

#### 7.3 å®Œæ•´è´å¶æ–¯å†³ç­–å™¨ä»£ç å®ç°

æ•´ä½“å®ç°ä»£ç å¦‚ä¸‹ï¼š

```python
'''
NaiveBayesClassifier.py
'''

class NaiveBayesClassifier:
    def __init__(self, alpha):
        self.alpha = alpha
        self.class_prior = {}
        self.cond_prob = {}

    def fit(self, X, y):
        self.classes = np.unique(y)
        self.features = [np.unique(X.iloc[:, col]) for col in range(X.shape[1])]
        self.X = X
        self.y = y
        total_count = len(y)
        for cls in self.classes:
            cls_count = np.sum(y == cls)
            self.class_prior[cls] = (cls_count + self.alpha) / (total_count + len(self.classes) * self.alpha)
            self.cond_prob[cls] = {}
            for i, feature in enumerate(self.features):
                self.cond_prob[cls][i] = {}
                for value in feature:
                    feature_count = np.sum((X.iloc[:, i] == value) & (y == cls))
                    self.cond_prob[cls][i][value] = (feature_count + self.alpha) / (
                                cls_count + len(feature) * self.alpha)

    def predict(self,X_test):
        predictions = []
        for x in X_test.values:
            probs ={}
            for cls in self.classes:
                probs[cls] = self.class_prior[cls]
                for i,value in enumerate(x):
                    if value in self.cond_prob[cls][i]:
                        probs[cls] *= self.cond_prob[cls][i][value]
                    else:
                        probs[cls] *= self.alpha / (np.sum(self.y == cls) + len(self.features[i]) * self.alpha)
            # print(max(probs, key=probs.get))
            predictions.append(max(probs, key=probs.get))
        return predictions
```

### 8.å®éªŒæ‰§è¡Œ

ç¼–å†™å®éªŒä¸»ç¨‹åº`exp1.py`

åˆ©ç”¨ä¸Šè¿°æè¿°çš„æœ€åŸºæœ¬çš„åˆ†æ®µï¼Œå³å‡åˆ†ä¸º14æ®µï¼Œæ‰§è¡Œä»£ç å¹¶è§‚æµ‹å‡†ç¡®ç‡

```python
'''
exp1.py
'''

import pandas as pd
from DataProcess import DataProcessor
from NaiveBayesClassifier import NaiveBayesClassifier

segmentMap = {
        'post_code': 14,
        'title': 14,
        'known_outstanding_loan': 14,
        'monthly_payment': 14,
        'issue_date': 14,
        'debt_loan_ratio': 14,
        'scoring_high': 14,
        'recircle_b': 14,
        'recircle_u': 14,
        'f0': 14,
        'f2': 14,
        'f3': 14,
        'early_return_amount_3mon': 14,
    }

train_data = pd.read_csv('../data/train.csv')
test_data = pd.read_csv('../data/test.csv')

dp = DataProcessor(train_data,segmentMap)

x_train, y_train = dp.Process()
x_test, y_test = dp.Deal(test_data)
# x_test.to_csv('../process/testData.csv', index=False)

nbc = NaiveBayesClassifier(alpha=1)
nbc.fit(x_train, y_train)
pred = nbc.predict(x_test)

correct_predictions = sum(p == t for p, t in zip(pred, y_test))
accuracy = correct_predictions / len(y_test)
print(f'Accuracy: {accuracy}')
```

è¿è¡Œåç»“æœå¦‚ä¸‹ï¼š

![](G:\ExpMachineLearn\ExpML\Exp1\pic\result\pre.png)

### 9.ä¼˜åŒ–è°ƒæ•´

åŸºäºä»¥ä¸Šçš„æ•°æ®å¤„ç†è¿‡ç¨‹ï¼Œèƒ½è¿›è¡Œä¼˜åŒ–çš„å³ç»†åŒ–å„ç±»åˆ«çš„åˆ†ç®±ä¸ªæ•°ã€‚åœ¨å®é™…æƒ…å†µä¸‹éœ€è¦è€ƒè™‘åˆ†ç®±çš„å¯è§£é‡Šæ€§ã€‚ç”±äºç¼ºä¹å®é™…çš„ç»æµç®¡ç†çŸ¥è¯†ï¼Œå¯¹æŸäº›ç‰¹å®šå€¼åˆ†ç®±çš„è§£é‡Šæ€§å¹¶ä¸èƒ½åšåˆ°å¾ˆå¥½ã€‚

å¦‚æœæŠŠåˆ†ç®±æ•°çœ‹ä½œè¶…å‚æ•°ï¼Œé€šè¿‡ä¸€äº›è¶…å‚æ•°æœç´¢ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨ç½‘æ ¼è°ƒå‚æœç´¢ã€éšæœºå‚æ•°æœç´¢ä»¥åŠé€€ç«ç­‰æ–¹æ³•ï¼Œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªæˆ–è€…ä¸€ç³»åˆ—å‡†ç¡®ç‡è¾ƒé«˜çš„åˆ†ç®±ä¸ªæ•°ï¼Œè¿™æ—¶ï¼Œå¯ä»¥å¯¹æ¯”éƒ¨åˆ†å˜åŒ–è¾ƒå¤§çš„ç®±æ•°æ¥æ¢ç©¶ä¸åŒçš„åˆ†ç®±ç­–ç•¥å¯¹æŸäº›ç‰¹å®šå€¼åˆ†ç®±çš„å¯è§†åŒ–æ•ˆæœã€‚

å…ˆåˆæ­¥è€ƒè™‘åˆ†ç®±çš„ä¼˜åŒ–æ–¹æ³•ï¼Œé¦–å…ˆå¯ä»¥é‡‡ç”¨æœç´¢çš„æ–¹æ³•å»åœ¨ä¸€å®šèŒƒå›´å†…ç©·ä¸¾æ‰€æœ‰çš„åˆ†ç®±æ•°ï¼Œä½†æ˜¯æ˜¾ç„¶å¦‚æœé‡‡ç”¨è¿™ç§æ–¹å¼ï¼Œæ•´ä¸ªä»£ç çš„è¿è¡Œæ—¶é—´æ˜¯é˜¶ä¹˜çº§çš„ï¼Œæ˜¯ä¸€ä¸ªNP-Hardé—®é¢˜ï¼ŒåŸºæœ¬æ˜¯ä¸å¯å–çš„ï¼Œå°¤å…¶æ˜¯åœ¨å¦‚ä¸Šæ–¹æ³•ç§å…±æœ‰14ä¸ªåˆ—éœ€è¦åˆ†ç®±ã€‚

#### 9.1 éšæœºæœç´¢

é¦–å…ˆè€ƒè™‘é‡‡ç”¨éšæœºæœç´¢çš„æ–¹æ³•ï¼Œæ¥å°è¯•æœç´¢æ›´å¥½çš„åˆ†ç®±ç»„æ•°ï¼Œé‡‡ç”¨å¦‚ä¸‹ä»£ç ï¼š

```python
'''
RandomSearch.py
éšæœºæœç´¢æŸ¥è¯¢æ›´å¥½çš„åˆ†ç®±æ•°ï¼Œä»¥æ­¤è¿½æ±‚æ›´é«˜çš„å‡†ç¡®ç‡
'''

import numpy as np
import pandas as pd

from Exp1.code.DataProcess import DataProcessor
from Exp1.code.NaiveBayesClassifier import NaiveBayesClassifier


train_data = pd.read_csv('../data/train.csv')
test_data = pd.read_csv('../data/test.csv')

segmentMap = {
        'post_code': 13,
        'title': 8,
        'known_outstanding_loan': 13,
        'monthly_payment': 20,
        'issue_date': 12,
        'debt_loan_ratio': 13,
        'scoring_low': 13,
        'scoring_high': 13,
        'recircle_b': 13,
        'recircle_u': 11,
        'f0': 11,
        'f2': 13,
        'f3': 13,
        'early_return_amount_3mon': 13,
}

def evaluate_model(parameters, dataT):
    # è¿™ä¸ªå‡½æ•°åº”è¯¥åŸºäºæä¾›çš„å‚æ•°å¤„ç†æ•°æ®å¹¶è¿”å›æ¨¡å‹å‡†ç¡®ç‡
    data = dataT.copy()
    dp = DataProcessor(data, parameters)
    x_train, y_train = dp.Process()  # å¤„ç†è®­ç»ƒæ•°æ®
    x_test, y_test = dp.Deal(test_data)  # å¤„ç†æµ‹è¯•æ•°æ®

    nbc = NaiveBayesClassifier(alpha=1)
    nbc.fit(x_train, y_train)
    predictions = nbc.predict(x_test)
    accuracy = np.mean(predictions == y_test)

    dp.Segment={}
    dp.isProcessed = False
    dp.data = None

    return accuracy


def random_search(data, iterations=100):
    best_params = None
    best_score = 0

    for _ in range(iterations):
        # éšæœºç”Ÿæˆå‚æ•°
        params = {key: np.random.randint(8, 21) for key in segmentMap.keys()}

        # è¯„ä¼°å½“å‰å‚æ•°é›†
        score = evaluate_model(params, data)

        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³å‚æ•°é›†
        if score > best_score:
            best_score = score
            best_params = params

        print(f"Current Params: {params} Score: {score}")

    return best_params, best_score


# è°ƒç”¨éšæœºæœç´¢
best_params, best_score = random_search(train_data)
print("Best Params:", best_params)
print("Best Score:", best_score)
```

å¯ä»¥å¾—åˆ°å¦‚ä¸‹çš„æœç´¢ç»“æœï¼š

ç”±äºæ¯æ¬¡çš„æœç´¢å­˜åœ¨å·®å¼‚æ€§ï¼Œå› æ­¤æ¯æ¬¡æœç´¢æœ€ä¼˜ç»“æœéƒ½ä¸ä¸€å®šä¸€è‡´ã€‚

![](G:\ExpMachineLearn\ExpML\Exp1\pic\result\RS100æ¬¡.png)

#### 9.2 æ¨¡æ‹Ÿé€€ç«æœç´¢

ä½†æ˜¯æ˜¾ç„¶è¿™ç§æœç´¢æ›´åƒæ˜¯æ¼«æ— ç›®çš„çš„ï¼Œå› æ­¤è€ƒè™‘æ•´ä½“è¶‹åŠ¿æ›´åŠ è‰¯å¥½çš„æ¨¡æ‹Ÿé€€ç«æ¥å¯¹è¿™äº›è¶…å‚æ•°è¿›è¡Œæœç´¢ä¼°è®¡ï¼ŒåŸºäºå¦‚ä¸‹ä»£ç å®ç°æ¨¡æ‹Ÿé€€ç«æœç´¢ï¼Œè¿›è¡Œè¶…å‚æœç´¢ã€‚

```python
'''
SimulatedAnnealing.py
æ¨¡æ‹Ÿé€€ç«æœç´¢æŸ¥è¯¢æ›´å¥½çš„åˆ†ç®±æ•°ï¼Œä»¥æ­¤è¿½æ±‚æ›´é«˜çš„å‡†ç¡®ç‡
'''

import numpy as np
import pandas as pd

from Exp1.code.DataProcess import DataProcessor
from Exp1.code.NaiveBayesClassifier import NaiveBayesClassifier

train_data = pd.read_csv('../data/train.csv')
test_data = pd.read_csv('../data/test.csv')

segmentMap = {
    'post_code': 13, 'title': 8, 'known_outstanding_loan': 13,
    'monthly_payment': 20, 'issue_date': 12, 'debt_loan_ratio': 13,
    'scoring_low': 13, 'scoring_high': 13, 'recircle_b': 13,
    'recircle_u': 11, 'f0': 11, 'f2': 13, 'f3': 13,
    'early_return_amount': 13, 'early_return_amount_3mon': 13,
}

def evaluate_model(parameters, dataT):
    data = dataT.copy()
    dp = DataProcessor(data, parameters)
    x_train, y_train = dp.Process()
    x_test, y_test = dp.Deal(test_data)

    nbc = NaiveBayesClassifier(alpha=1)
    nbc.fit(x_train, y_train)
    predictions = nbc.predict(x_test)
    accuracy = np.mean(predictions == y_test)

    return accuracy

def simulated_annealing(data, iterations=300, temp=1.0, temp_decay=0.95):
    current_params = {key: np.random.randint(8, 21) for key in segmentMap.keys()}
    current_score = evaluate_model(current_params, data)
    best_params = current_params.copy()
    best_score = current_score

    for i in range(iterations):
        new_params = current_params.copy()
        for key in new_params.keys():
            if np.random.rand() < 0.5:
                new_params[key] = np.random.randint(8, 21)

        new_score = evaluate_model(new_params, data)

        if new_score > current_score:
            accept = True
        else:
            delta = new_score - current_score
            accept_prob = np.exp(delta / temp)
            accept = np.random.rand() < accept_prob

        if accept:
            current_params, current_score = new_params, new_score
            if new_score > best_score:
                best_params, best_score = new_params.copy(), new_score

        temp *= temp_decay

        print(f"Iteration {i+1}: Current Params: {current_params}, Score: {current_score}, Temp: {temp}")

    return best_params, best_score

best_params, best_score = simulated_annealing(train_data)
print("Best Params:", best_params)
print("Best Score:", best_score)
```

æœç´¢ç»“æœå¦‚ä¸‹ï¼š

ç”±äºæ¯æ¬¡çš„æœç´¢å­˜åœ¨å·®å¼‚æ€§ï¼Œå› æ­¤æ¯æ¬¡æœç´¢æœ€ä¼˜ç»“æœéƒ½ä¸ä¸€å®šä¸€è‡´ã€‚

![](G:\ExpMachineLearn\ExpML\Exp1\pic\result\SL500æ¬¡.png)

#### 9.3 æ¨¡æ‹Ÿé€€ç«+K-Fold(k=5)

è€ƒè™‘åˆ°ä¸Šè¿°çš„æ¨¡å‹éƒ½æ˜¯åˆ©ç”¨æµ‹è¯•é›†æ¥è¿›è¡Œï¼Œè¿™ç¼ºä¹ä¸€äº›æ³›åŒ–æ€§ï¼Œå¯¹è®­ç»ƒé›†é‡‡ç”¨k-Foldï¼Œé‡‡ç”¨5ä¸ªå 

```python
import numpy as np
import pandas as pd

from Exp1.code.DataProcess import DataProcessor
from Exp1.code.NaiveBayesClassifier import NaiveBayesClassifier

train_data = pd.read_csv('../data/train.csv')
test_data = pd.read_csv('../data/test.csv')

segmentMap = {
    'post_code': 13, 'title': 8, 'known_outstanding_loan': 13,
    'monthly_payment': 20, 'issue_date': 12, 'debt_loan_ratio': 13,
    'scoring_low': 13, 'scoring_high': 13, 'recircle_b': 13,
    'recircle_u': 11, 'f0': 11, 'f2': 13, 'f3': 13,
    'early_return_amount': 13, 'early_return_amount_3mon': 13,
}

def evaluate_model(parameters, data, n_folds=5):
    accuracies = []

    # è®¡ç®—æ¯ä¸ªæŠ˜çš„å¤§å°
    fold_size = len(data) // n_folds

    for i in range(n_folds):
        # ç¡®å®šéªŒè¯é›†çš„ç´¢å¼•èŒƒå›´
        start_idx = i * fold_size
        end_idx = (i + 1) * fold_size if i < n_folds - 1 else len(data)

        # åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        validation_data = data.iloc[start_idx:end_idx]
        train_data = pd.concat([data.iloc[:start_idx], data.iloc[end_idx:]])

        dp = DataProcessor(train_data, parameters)
        x_train, y_train = dp.Process()
        x_test, y_test = dp.Deal(validation_data)

        # è®­ç»ƒæ¨¡å‹
        nbc = NaiveBayesClassifier(alpha=1)
        nbc.fit(x_train, y_train)

        # è¿›è¡Œé¢„æµ‹å¹¶è®¡ç®—å‡†ç¡®ç‡
        predictions = nbc.predict(x_test)
        accuracy = np.mean(predictions == y_test)
        accuracies.append(accuracy)

    mean_accuracy = np.mean(accuracies)
    return mean_accuracy

def simulated_annealing(data, iterations=300, temp=1.0, temp_decay=0.95):
    current_params = {key: np.random.randint(8, 21) for key in segmentMap.keys()}
    current_score = evaluate_model(current_params, data)
    best_params = current_params.copy()
    best_score = current_score

    for i in range(iterations):
        new_params = current_params.copy()
        for key in new_params.keys():
            if np.random.rand() < 0.5:
                new_params[key] = np.random.randint(8, 21)

        new_score = evaluate_model(new_params, data)

        if new_score > current_score:
            accept = True
        else:
            delta = new_score - current_score
            accept_prob = np.exp(delta / temp)
            accept = np.random.rand() < accept_prob

        if accept:
            current_params, current_score = new_params, new_score
            if new_score > best_score:
                best_params, best_score = new_params.copy(), new_score

        temp *= temp_decay

        print(f"Iteration {i+1}: Current Params: {current_params}, Score: {current_score}, Temp: {temp}")

    return best_params, best_score

best_params, best_score = simulated_annealing(train_data)
print("Best Params:", best_params)
print("Best Score:", best_score)
```

æœç´¢ç»“æœå¦‚ä¸‹ï¼š

æ˜¾ç„¶è€ƒè™‘æ›´å¤šæ•ˆæœæ—¶çš„å‡†ç¡®ç‡ä¸å¦‚ä¹‹å‰ä¸¤ç§ï¼Œä½†è¿™ç§çš„æ³›åŒ–æ€§æ›´å¥½ï¼Œè¿‡æ‹Ÿåˆçš„æƒ…å†µæ›´å°‘ã€‚ä½†è¿­ä»£300æ¬¡çš„æ—¶å€™çš„è¿è¡Œé€Ÿåº¦éå¸¸çš„ç¼“æ…¢ã€‚

![](G:\ExpMachineLearn\ExpML\Exp1\pic\result\kSL300æ¬¡.png)

åˆ©ç”¨è¿™ä¸ªåˆ†æ®µæ–¹å¼æ¥æ±‚æµ‹è¯•é›†çš„å‡†ç¡®ç‡å¦‚ä¸‹ï¼š

![](G:\ExpMachineLearn\ExpML\Exp1\pic\result\kFold.png)



### 10.ä»£ç ç›®å½•ç»“æ„

ä»£ç çš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
/Exp1/
|------- code/
|		|------- DataAnaslyze.py æ•°æ®åˆ†æçš„å‡½æ•°ä¸æ•´ä½“åˆ†æ
|		|------- DataProcess.py æ•°æ®å¤„ç†å™¨
|		|------- NaiveBayesClassifier.py è´å¶æ–¯å†³ç­–å™¨
|		|------- exp1.py å®éªŒä¸€ä¸»ä»£ç 
|		|------- RandomSearch.py éšæœºæœç´¢åˆ†ç®±æ•°
|		|------- SimulatedAnnealing.py æ¨¡æ‹Ÿé€€ç«æœç´¢
|		|------- kFoldandSL.py kæŠ˜å æ¨¡æ‹Ÿé€€ç«
|
|-------- data/
|		|------- train.csv è®­ç»ƒæ•°æ®
|		|------- test.csv æµ‹è¯•æ•°æ®
|		|------- æ•°æ®åˆ†æå™¨.xlsx æ•°æ®åˆ†æè¡¨æ ¼
|		|------- ç›¸å…³ç³»æ•°çŸ©é˜µ.xlsx 
|
|-------- pic/
|		|------- res/ å®éªŒæŠ¥å‘Šæœ‰å…³å›¾ç‰‡
|		|------- å…¶ä»–ä¿¡æ¯
|
|-------- process/
|		|------- Processed_TrainData.csv ä¸´æ—¶å¤„ç†æ•°æ®
|		|------- test.csv ä¸´æ—¶å¤„ç†çš„æ•°æ®
|
|-------- Exp1.md å®éªŒæŠ¥å‘ŠMarkdown
|
|-------- Exp1.pdf å®éªŒæŠ¥å‘Špdf
```

## å¿ƒå¾—ä½“ä¼š

åœ¨è¿™æ¬¡å®éªŒä¸­ï¼Œæˆ‘è‡´åŠ›äºé€šè¿‡æ‰‹åŠ¨ç¼–å†™ä»£ç ï¼Œå¹¶ä»…ä½¿ç”¨NumPyå’ŒPandasåº“ï¼Œæ·±å…¥ç†è§£æœºå™¨å­¦ä¹ åŸºç¡€åŸç†ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ•°æ®å¤„ç†å’Œå»ºæ¨¡è¿‡ç¨‹ä¸­ã€‚

é¦–å…ˆï¼Œæˆ‘è¿›è¡Œäº†å¯¹æ•°æ®çš„ç®€å•åˆ†æï¼Œä»¥ä¾¿å…¨é¢äº†è§£æ•°æ®çš„ç‰¹å¾å’Œç»“æ„ã€‚æ¥ç€ï¼Œæˆ‘å¤„ç†äº†æ•°æ®ä¸­çš„ç¼ºå¤±å€¼ï¼Œç¡®ä¿äº†æ•°æ®çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚é€šè¿‡æ•°æ®è½¬æ¢æ“ä½œï¼Œæˆ‘å°†æ–‡æœ¬ç±»å‹å’Œæ—¥æœŸç±»å‹çš„æ•°æ®è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œä»¥ä¾¿åç»­çš„å»ºæ¨¡å’Œåˆ†æå·¥ä½œã€‚

åœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘é‡‡å–äº†æ•°æ®åˆ å‡çš„ç­–ç•¥ï¼Œå»é™¤äº†å¯¹å»ºæ¨¡æ— æ„ä¹‰æˆ–å†—ä½™çš„ç‰¹å¾ï¼Œç®€åŒ–äº†æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘è¿›è¡Œäº†æ•°æ®åˆ†ç®±æ“ä½œï¼Œå°†è¿ç»­å‹æ•°æ®ç¦»æ•£åŒ–ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹æ•°æ®åˆ†å¸ƒçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚

åœ¨æ¨¡å‹å»ºç«‹æ–¹é¢ï¼Œæˆ‘å®ç°äº†è´å¶æ–¯å†³ç­–å™¨ï¼Œå¹¶å¯¹å…¶ç›¸å…³ç†è®ºè¿›è¡Œäº†ç®€è¦é˜è¿°ã€‚é€šè¿‡ä»£ç å®ç°äº†æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç­‰æŠ€æœ¯ï¼Œæå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºåç»­çš„åˆ†ç±»ä»»åŠ¡æä¾›äº†å¯é çš„åŸºç¡€ã€‚

åœ¨å®éªŒæ‰§è¡Œé˜¶æ®µï¼Œæˆ‘é‡‡ç”¨äº†å¤šç§ä¼˜åŒ–è°ƒæ•´æ–¹æ³•ï¼ŒåŒ…æ‹¬éšæœºæœç´¢ã€æ¨¡æ‹Ÿé€€ç«æœç´¢ä»¥åŠæ¨¡æ‹Ÿé€€ç«ç»“åˆK-Foldäº¤å‰éªŒè¯ç­‰æŠ€æœ¯ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†å¯é çš„æ”¯æŒã€‚

æœ€åï¼Œé€šè¿‡æ•´ç†ä»£ç ç›®å½•ç»“æ„ï¼Œä½¿å¾—ä»£ç å…·æœ‰è‰¯å¥½çš„ç»„ç»‡ç»“æ„å’Œå¯è¯»æ€§ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘æ·±å…¥ç†è§£äº†æœºå™¨å­¦ä¹ ä¸­çš„åŸºç¡€æ¦‚å¿µå’Œå¸¸ç”¨æŠ€æœ¯ï¼Œå¹¶é€šè¿‡å®è·µåŠ æ·±äº†å¯¹è¿™äº›æŠ€æœ¯çš„ç†è§£å’ŒæŒæ¡ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™æ¬¡å®éªŒä¸ä»…åŠ æ·±äº†æˆ‘çš„å¯¹æœºå™¨å­¦ä¹ åŸºç¡€çŸ¥è¯†çš„ç†è§£ï¼Œä¹Ÿæé«˜äº†æˆ‘çš„ç¼–ç¨‹å’Œæ•°æ®å¤„ç†èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰ç›Šçš„å­¦ä¹ ç»å†ï¼Œä¸ºæˆ‘æœªæ¥åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„è¿›ä¸€æ­¥æ¢ç´¢å’Œå®è·µå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

